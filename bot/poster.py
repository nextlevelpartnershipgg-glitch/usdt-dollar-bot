import os, io, json, time, textwrap, pathlib, hashlib, urllib.parse, sys, random, re
from datetime import datetime, timezone, timedelta
from dateutil import parser as dtparse
import feedparser, requests
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
from zoneinfo import ZoneInfo

# ============ –ù–ê–°–¢–†–û–ô–ö–ò ============
BOT_TOKEN  = os.environ.get("BOT_TOKEN")                   # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —á–µ—Ä–µ–∑ GitHub Secrets
CHANNEL_ID = os.environ.get("CHANNEL_ID", "@usdtdollarm")  # —Ç–≤–æ–π –∫–∞–Ω–∞–ª –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
TIMEZONE   = os.environ.get("TIMEZONE", "Europe/Zurich")   # —Ç–≤–æ–π —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å

# –°–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–∏—Ç—å –∑–∞ –∑–∞–ø—É—Å–∫ (–∞–Ω—Ç–∏-—Å–ø–∞–º)
MAX_POSTS_PER_RUN = int(os.environ.get("MAX_POSTS_PER_RUN", "5"))
# –° –∫–∞–∫–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ –±—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ (–º–∏–Ω—É—Ç)
LOOKBACK_MINUTES  = int(os.environ.get("LOOKBACK_MINUTES", "90"))

# –†—É—Å—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = [
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",  # –†–ë–ö
    "https://lenta.ru/rss/news",                          # Lenta.ru
    "https://www.gazeta.ru/export/rss/lenta.xml",         # –ì–∞–∑–µ—Ç–∞.ru
    "https://tass.ru/rss/v2.xml",                         # –¢–ê–°–°
    "https://www.kommersant.ru/RSS/news.xml",             # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä
]

# –•—ç—à—Ç–µ–≥–∏
TAGS = "#–Ω–æ–≤–æ—Å—Ç–∏ #—Ä—ã–Ω–∫–∏ #—ç–∫–æ–Ω–æ–º–∏–∫–∞ #–∞–∫—Ü–∏–∏ #usdt #–¥–æ–ª–ª–∞—Ä"

# –§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤)
DATA_DIR = pathlib.Path("data"); DATA_DIR.mkdir(parents=True, exist_ok=True)
STATE_FILE = DATA_DIR / "state.json"

UA = {"User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}

# ===== –£–¢–ò–õ–ò–¢–´ =====
def load_state():
    if STATE_FILE.exists():
        return json.loads(STATE_FILE.read_text(encoding="utf-8"))
    return {}

def save_state(state):
    STATE_FILE.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def domain(url):
    return urllib.parse.urlparse(url).netloc.replace("www.", "") or "source"

def clean_html(html):
    if not html:
        return ""
    soup = BeautifulSoup(html, "html.parser")
    return " ".join(soup.get_text(separator=" ").split())

def clamp(s, n):
    s = (s or "").strip()
    return (s if len(s) <= n else s[:n-1] + "‚Ä¶")

def make_caption(title, summary, link, ctx_lines=None):
    title = clamp(title, 200)
    summary = clamp(summary, 750)  # –¥–µ–ª–∞–µ–º –¥–ª–∏–Ω–Ω–µ–µ –¥–ª—è ¬´—Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ¬ª
    lines = [f"üíµ {title}", f"‚Äî {summary}"]
    if ctx_lines:
        lines += ["", "üß≠ –ö–æ–Ω—Ç–µ–∫—Å—Ç:"] + ctx_lines
    lines += ["", f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {link}", TAGS]
    cap = "\n".join(lines)
    # –ª–∏–º–∏—Ç –ø–æ–¥–ø–∏—Å–∏ Telegram ~1024
    if len(cap) > 1024:
        # —É—Ä–µ–∂–µ–º summary
        shrink = len(cap) - 1024 + 3
        summary2 = clamp(summary[:-shrink], 730)
        lines[1] = f"‚Äî {summary2}"
        cap = "\n".join(lines)
    return cap

# ---------- –§–û–ù: Unsplash ‚Üí Picsum ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç ----------
def fetch_unsplash_image(query, w=1080, h=540, retries=3):
    for i in range(retries):
        try:
            seed = random.randint(0, 10_000_000)
            url = f"https://source.unsplash.com/{w}x{h}/?{urllib.parse.quote(query)}&sig={seed}"
            r = requests.get(url, headers=UA, timeout=25, allow_redirects=True)
            if r.status_code == 200:
                return Image.open(io.BytesIO(r.content)).convert("RGB")
            time.sleep(0.8 * (i+1))
        except Exception:
            time.sleep(0.8 * (i+1))
    return None

def fetch_picsum_image(w=1080, h=540):
    try:
        seed = random.randint(1, 10_000_000)
        url = f"https://picsum.photos/{w}/{h}?random={seed}"
        r = requests.get(url, headers=UA, timeout=20, allow_redirects=True)
        if r.status_code == 200:
            return Image.open(io.BytesIO(r.content)).convert("RGB")
    except Exception:
        pass
    return None

def gradient_fallback(w=1080, h=540):
    top = (24, 26, 28); bottom = (10, 12, 14)
    img = Image.new("RGB", (w, h))
    draw = ImageDraw.Draw(img)
    for y in range(h):
        alpha = y / (h-1)
        r = int(top[0]*(1-alpha) + bottom[0]*alpha)
        g = int(top[1]*(1-alpha) + bottom[1]*alpha)
        b = int(top[2]*(1-alpha) + bottom[2]*alpha)
        draw.line([(0,y),(w,y)], fill=(r,g,b))
    return img

KEYMAP = [
    (["—Ñ—Ä—Å","—Å—Ç–∞–≤–∫","–∏–Ω—Ñ–ª—è—Ü","cpi","ppi","–ø—Ä–æ—Ü–µ–Ω—Ç"], "interest rates,economy,bank"),
    (["–Ω–µ—Ñ—Ç—å","–±—Ä–µ–Ω—Ç","wti","oil","–æ–ø–µ–∫"], "oil,barrels,energy,refinery"),
    (["–≥–∞–∑","lng","–≥–∞–∑–æ–ø—Ä–æ–≤–æ–¥"], "natural gas,energy,pipeline"),
    (["—Ä—É–±–ª","ruble","—Ä—É–±"], "ruble,currency,money"),
    (["–¥–æ–ª–ª–∞—Ä","usd","dxy","usdt"], "dollar,currency,finance,wall street"),
    (["–±–∏—Ç–∫–æ–∏–Ω","bitcoin","btc","–∫—Ä–∏–ø—Ç","crypto","ether","eth"], "crypto,blockchain,bitcoin,ethereum"),
    (["–∞–∫—Ü–∏","–∏–Ω–¥–µ–∫—Å","s&p","nasdaq","—Ä—ã–Ω–æ–∫","–±–∏—Ä–∂–∞"], "stocks,stock market,ticker,wall street"),
    (["–µ–≤—Ä–æ","eur"], "euro,currency,finance"),
    (["–∑–æ–ª–æ—Ç–æ","gold","xau"], "gold,precious metal,ingots"),
]

def pick_photo_query(title, summary):
    text = f"{title} {summary}".lower()
    for keys, q in KEYMAP:
        if any(k in text for k in keys):
            return q
    return "finance,markets,city night,news"

def get_background(title, summary, w=1080, h=540):
    q = pick_photo_query(title, summary)
    img = fetch_unsplash_image(q, w, h) or fetch_picsum_image(w, h) or gradient_fallback(w, h)
    img = img.filter(ImageFilter.GaussianBlur(radius=0.6))
    img = ImageEnhance.Brightness(img).enhance(0.85)
    return img

# ---------- –ö–ê–†–¢–û–ß–ö–ê 1080x540 ----------
def draw_card_quote(title_text, summary_text, src_domain, tzname):
    W, H = 1080, 540
    bg = get_background(title_text, summary_text, W, H)

    # –∑–∞—Ç–µ–º–Ω—è–µ–º –ø–æ–¥ —Ç–µ–∫—Å—Ç
    overlay = Image.new("RGBA", (W, H), (0,0,0,0))
    od = ImageDraw.Draw(overlay)
    od.rounded_rectangle([40, 90, W-40, H-70], radius=28, fill=(0,0,0,120))
    bg = Image.alpha_composite(bg.convert("RGBA"), overlay).convert("RGB")
    d = ImageDraw.Draw(bg)

    # –®—Ä–∏—Ñ—Ç—ã
    font_brand     = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 34)
    font_time      = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 26)
    font_quote     = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 40)
    font_title     = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 50)
    font_small     = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 24)
    font_quote_mark= ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 80)

    # –í–µ—Ä—Ö: –±—Ä–µ–Ω–¥ + –≤—Ä–µ–º—è
    brand = "USDT=Dollar"
    d.text((48, 26), brand, fill=(255,255,255), font=font_brand)
    try:
        tz = ZoneInfo(tzname)
    except Exception:
        tz = ZoneInfo("UTC")
    now_str = datetime.now(tz).strftime("%d.%m %H:%M")
    d.text((W - 48 - d.textlength(now_str, font=font_time), 26), now_str, fill=(255,255,255), font=font_time)

    # –¢–µ–ª–æ
    margin_x = 72
    y = 120
    d.text((margin_x - 20, y - 20), "‚Äú", fill=(255,255,255), font=font_quote_mark)

    for line in textwrap.wrap((title_text or "").strip(), width=28)[:3]:
        d.text((margin_x + 50, y), line, font=font_title, fill=(255,255,255))
        y += 58

    short = clamp((summary_text or "").strip().replace("\n", " "), 380)
    if short:
        y += 12
        for ln in textwrap.wrap(short, width=40):
            if y + 42 > H - 100:
                break
            d.text((margin_x + 50, y), ln, font=font_quote, fill=(230,230,230))
            y += 42

    d.text((W - 110, H - 140), "‚Äù", fill=(255,255,255), font=font_quote_mark)

    # –ù–∏–∑: –∏—Å—Ç–æ—á–Ω–∏–∫
    src = f"source: {src_domain}"
    d.text((72, H - 56), src, font=font_small, fill=(220,220,220))

    bio = io.BytesIO()
    bg.save(bio, format="PNG", optimize=True)
    bio.seek(0)
    return bio

def send_photo(photo_bytes, caption):
    if not BOT_TOKEN:
        raise RuntimeError("–ù–µ—Ç BOT_TOKEN. –î–æ–±–∞–≤—å —Å–µ–∫—Ä–µ—Ç –≤ GitHub: Settings ‚Üí Secrets ‚Üí Actions ‚Üí BOT_TOKEN")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto"
    files = {"photo": ("cover.png", photo_bytes, "image/png")}
    data = {"chat_id": CHANNEL_ID, "caption": caption}
    r = requests.post(url, files=files, data=data, timeout=30)
    print("Telegram status:", r.status_code, r.text[:200])
    r.raise_for_status()
    return r.json()

# ---------- –ü–û–õ–£–ß–ï–ù–ò–ï –†–ê–ó–í–Å–†–ù–£–¢–û–ì–û –¢–ï–ö–°–¢–ê –ò–ó –°–¢–ê–¢–¨–ò ----------
def fetch_article_text(url, max_chars=1200):
    try:
        r = requests.get(url, headers=UA, timeout=20)
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        # –±–µ—Ä—ë–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∞–±–∑–∞—Ü—ã
        ps = soup.find_all("p")
        chunks = []
        for p in ps:
            t = p.get_text(" ", strip=True)
            if not t: continue
            if len(t) < 60:  # –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ–¥–ø–∏—Å–∏
                continue
            # –æ—Ç—Å–µ–∫–∞–µ–º –º—É—Å–æ—Ä
            if any(x in t.lower() for x in ["javascript", "cookie", "–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å", "—Ä–µ–∫–ª–∞–º–∞", "cookies"]):
                continue
            chunks.append(t)
            if sum(len(c) for c in chunks) > max_chars:
                break
        text = " ".join(chunks)
        # —É–ø–ª–æ—Ç–Ω–∏–º
        text = re.sub(r"\s+", " ", text).strip()
        return text
    except Exception:
        return ""

def expanded_summary(feed_summary, article_text, limit=900):
    # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏, –∑–∞—Ç–µ–º —Ñ–∏–¥
    base = (article_text or "").strip()
    if not base:
        base = (feed_summary or "").strip()
    # –≤–æ–∑—å–º—ë–º 3‚Äì4 –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sents = re.split(r"(?<=[.!?])\s+", base)
    out = " ".join(sents[:4]).strip()
    return clamp(out, limit)

# ---------- –°–¢–†–ê–ù–ê + –ì–û–°.–õ–ò–¶–ê ----------
# –£–ø—Ä–æ—â—ë–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (RU/EN)
COUNTRIES = [
    ("–†–æ—Å—Å–∏—è", ["—Ä–æ—Å—Å–∏—è","—Ä—Ñ","–º–æ—Å–∫–≤–∞","—Ä—É–±–ª","–ø—É—Ç–∏", "—Ä–æ—Å—Å–∏–π"]),          "Q159",
    ("–°–®–ê",    ["—Å—à–∞","—Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —à—Ç","washington","–±–∞–π–¥–µ–Ω","–¥–æ–ª–ª–∞—Ä","—Ñ—Ä—Å","–±–µ–ª—ã–π –¥–æ–º"]), "Q30",
    ("–ö–∏—Ç–∞–π",  ["–∫–∏—Ç–∞–π","–∫–Ω—Ä","–ø–µ–∫–∏–Ω","—Å–∏ —Ü–∑–∏–Ω—å–ø–∏–Ω","—à–∞–Ω—å—Ö–∞–π","yuan","cny"]),              "Q148",
    ("–£–∫—Ä–∞–∏–Ω–∞",["—É–∫—Ä–∞–∏–Ω","–∫–∏–µ–≤","kyiv","–∑–µ–ª–µ–Ω—Å–∫","–≥—Ä–∏–≤–Ω","uah"]),                           "Q212",
    ("–ì–µ—Ä–º–∞–Ω–∏—è",["–≥–µ—Ä–º–∞–Ω","–±–µ—Ä–ª–∏–Ω","scholz","–µ–≤—Ä–æ","bundes"]),                              "Q183",
    ("–§—Ä–∞–Ω—Ü–∏—è",["—Ñ—Ä–∞–Ω—Ü","–ø–∞—Ä–∏–∂","–º–∞–∫—Ä–æ–Ω","euro","elysee"]),                                 "Q142",
    ("–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è",["–±—Ä–∏—Ç–∞–Ω–∏","–±—Ä–∏—Ç–∞–Ω","–ª–æ–Ω–¥–æ–Ω","–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç","uk","king charles","–ø—Ä–µ–º—å–µ—Ä"], "Q145"),
    ("–ò—Ç–∞–ª–∏—è",["–∏—Ç–∞–ª","—Ä–∏–º","meloni","euro","–∏—Ç–∞–ª—å—è–Ω"],                                     "Q38"),
    ("–ò—Å–ø–∞–Ω–∏—è",["–∏—Å–ø–∞–Ω","–º–∞–¥—Ä–∏–¥","sanchez","euro","–∏–±–µ—Ä–∏"],                                 "Q29"),
    ("–Ø–ø–æ–Ω–∏—è",["—è–ø–æ–Ω–∏","—Ç–æ–∫–∏–æ","yen","jpy","kishida"],                                      "Q17"),
    ("–ò–Ω–¥–∏—è",["–∏–Ω–¥–∏—è","–Ω—å—é-–¥–µ–ª–∏","rupee","modi","inr"],                                     "Q668"),
    ("–¢—É—Ä—Ü–∏—è",["—Ç—É—Ä—Ü–∏","–∞–Ω–∫–∞—Ä–∞","—ç—Ä–¥–æ–≥–∞–Ω","lira","try"]),                                   "Q43"),
    ("–ü–æ–ª—å—à–∞",["–ø–æ–ª—å—à","–≤–∞—Ä—à–∞–≤–∞","zl","pln","—Ç–æ—Å–∫","tusk"]),                                "Q36"),
    ("–ë–µ–ª–∞—Ä—É—Å—å",["–±–µ–ª–∞—Ä—É—Å","–º–∏–Ω—Å–∫","–ª—É–∫–∞—à–µ–Ω–∫","byn","–±–µ–ª–æ—Ä—É—Å"],                             "Q184"),
    ("–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω",["–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω","–∞—Å—Ç–∞–Ω","—Ç–µ–Ω–≥–µ","kzt","—Ç–æ–∫–∞–µ–≤"],                              "Q232"),
    ("–ò—Ä–∞–Ω",["–∏—Ä–∞–Ω","—Ç–µ–≥–µ—Ä–∞–Ω","rial","irn","—Ä–∞–∏—Å–∏","—Ö–∞–º–µ–Ω–µ"],                               "Q794"),
    ("–ò–∑—Ä–∞–∏–ª—å",["–∏–∑—Ä–∞–∏–ª","—Ç–µ–ª—å-–∞–≤–∏–≤","–Ω–µ—Ç–∞–Ω—å—è—Ö—É","—à–µ–∫–µ–ª","ils"]),                           "Q801"),
    ("–û–ê–≠",["–æ–∞—ç","—ç–º–∏—Ä–∞—Ç","–∞–±—É-–¥–∞–±–∏","–¥—É–±–∞–π","aed","dirham"]),                             "Q878"),
    ("–°–∞—É–¥–æ–≤—Å–∫–∞—è –ê—Ä–∞–≤–∏—è",["—Å–∞—É–¥","—Ä–∏–∞–¥","sar","saudi","–º–±—Å"]),                               "Q851"),
    ("–ö–∞–Ω–∞–¥–∞",["–∫–∞–Ω–∞–¥–∞","–æ—Ç—Ç–∞–≤–∞","cad","trudeau"]),                                         "Q16"),
    ("–ë—Ä–∞–∑–∏–ª–∏—è",["–±—Ä–∞–∑–∏–ª","–±—Ä–µ","—Ä–µ–∞–ª","lula","rio","sao paulo"]),                          "Q155"),
    ("–ú–µ–∫—Å–∏–∫–∞",["–º–µ–∫—Å–∏–∫","–ø–µ—Å–æ","mxn","–æ–±—Ä–∞–¥–æ—Ä","lopez obrador"]),                          "Q96"),
]

def detect_country(text):
    t = (text or "").lower()
    for name, keys, q in [(n,k,q) for (n,k), q in [(x[:2], x[2]) for x in [ (c[0],c[1],c[2]) for c in [(a[0], a[1], a[2]) for a in [(*c, ) if len(c)==3 else c for c in [(c[0], c[1], c[2]) if len(c)==3 else (c[0], c[1], "Q0") for c in [ (*c, ) for c in COUNTRIES ]]]]]]:
        pass  # (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è; –Ω–∏–∂–µ ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)

# –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è, —á–∏—Ç–∞–±–µ–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è detect_country (–≤—ã—à–µ –æ—Å—Ç–∞–≤–ª–µ–Ω ¬´pass¬ª, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—É—Ç–∞–Ω–∏—Ü—ã)
def detect_country(text2):
    t = (text2 or "").lower()
    for entry in COUNTRIES:
        name, keys, qid = entry
        if any(k in t for k in keys):
            return {"name": name, "qid": qid}
    return None

def wikidata_officials(qid):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (head_of_state, head_of_gov). –ë–µ–∑ –ø–∞–¥–µ–Ω–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö."""
    try:
        query = f"""
        SELECT ?hosLabel ?hogLabel WHERE {{
          OPTIONAL {{ wd:{qid} wdt:P35 ?hos. }}
          OPTIONAL {{ wd:{qid} wdt:P6  ?hog. }}
          SERVICE wikibase:label {{ bd:serviceParam wikibase:language "ru,en". }}
        }}
        """
        r = requests.get(
            "https://query.wikidata.org/sparql",
            params={"query": query, "format": "json"},
            headers={"Accept":"application/sparql-results+json","User-Agent":"usdtdollar-bot/1.0"}
        , timeout=15)
        if r.status_code != 200:
            return (None, None)
        data = r.json()
        hos, hog = None, None
        for b in data.get("results", {}).get("bindings", []):
            if "hosLabel" in b and not hos:
                hos = b["hosLabel"]["value"]
            if "hogLabel" in b and not hog:
                hog = b["hogLabel"]["value"]
        return (hos, hog)
    except Exception:
        return (None, None)

# ---------- –°–ë–û–† –ò –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø ----------
def collect_entries():
    items = []
    for feed_url in RSS_FEEDS:
        fp = feedparser.parse(feed_url)
        for e in fp.entries or []:
            link = getattr(e, "link", "") or ""
            title = (getattr(e, "title", "") or "").strip()
            summary = clean_html(getattr(e, "summary", getattr(e, "description", "")))
            ts = getattr(e, "published", getattr(e, "updated", "")) or ""
            try:
                dt = dtparse.parse(ts)
                if not dt.tzinfo:
                    dt = dt.replace(tzinfo=timezone.utc)
            except Exception:
                dt = datetime(1970,1,1, tzinfo=timezone.utc)
            uid = hashlib.sha256((link + "|" + title + "|" + ts).encode("utf-8")).hexdigest()
            items.append({
                "feed": feed_url,
                "link": link,
                "title": title or "(no title)",
                "summary": summary,
                "ts": ts,
                "dt": dt,
                "uid": uid,
            })
    return items

def process_item(link, title, feed_summary):
    # —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Å—Ç–∞—Ç—å–∏
    article_text = fetch_article_text(link, max_chars=1600)
    long_summary = expanded_summary(feed_summary, article_text, limit=900)

    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω—É –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É + —Ç–µ–∫—Å—Ç—É
    country_info = detect_country(f"{title} {feed_summary} {article_text}")
    ctx_lines = []
    if country_info:
        hos, hog = wikidata_officials(country_info["qid"])
        ctx_lines.append(f"üó∫Ô∏è –°—Ç—Ä–∞–Ω–∞: {country_info['name']}")
        if hos: ctx_lines.append(f"üë§ –ì–ª–∞–≤–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞: {hos}")
        if hog: ctx_lines.append(f"üë§ –ì–ª–∞–≤–∞ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {hog}")

    # –ø–æ–¥–ø–∏—Å—å –∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞
    cap  = make_caption(title, long_summary, link or "", ctx_lines=ctx_lines)
    card = draw_card_quote(title, long_summary, domain(link or ""), TIMEZONE)

    resp = send_photo(card, cap)
    print("Posted:", (title or "")[:80], "‚Üí", resp.get("ok", True))

def trim_posted(posted_set, keep_last=600):
    if len(posted_set) <= keep_last:
        return posted_set
    return set(list(posted_set)[-keep_last:])

def main():
    state = load_state()
    posted = set(state.get("posted_uids", []))

    items = collect_entries()
    if not items:
        print("No entries found.")
        return

    now = datetime.now(timezone.utc)
    lookback_dt = now - timedelta(minutes=LOOKBACK_MINUTES)
    fresh = [it for it in items if it["dt"] >= lookback_dt and it["uid"] not in posted]

    fresh.sort(key=lambda x: x["dt"], reverse=True)
    to_post = fresh[:MAX_POSTS_PER_RUN]

    if not to_post:
        print("Nothing new to post within lookback window.")
        return

    for it in to_post:
        try:
            process_item(it["link"], it["title"], it["summary"])
            posted.add(it["uid"])
            time.sleep(1.0)
        except Exception as e:
            print("Error sending:", e)

    state["posted_uids"] = list(trim_posted(posted))
    save_state(state)

if __name__ == "__main__":
    main()
