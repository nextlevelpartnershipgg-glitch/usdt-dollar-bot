# -*- coding: utf-8 -*-
"""
USDT=Dollar ‚Äî –∞–≤—Ç–æ-–ø–æ—Å—Ç–µ—Ä –¥–ª—è –∫–∞–Ω–∞–ª–∞ (RU only).
‚Äî –ß–∏—Ç–∞–µ—Ç 50 —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö RSS (data/sources_ru.txt)
‚Äî –î–ª—è —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤—ã—Ç—è–≥–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç, —á–∏—Å—Ç–∏—Ç, —á–∞—Å—Ç–∏—á–Ω–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç (~50%), –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤
‚Äî –§–∏–ª—å—Ç—Ä—ã: —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞, –º–∏–Ω–∏–º—É–º 400 —Å–∏–º–≤–æ–ª–æ–≤, —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
‚Äî –†–∏—Å—É–µ—Ç –º—è–≥–∫—É—é –æ–±–ª–æ–∂–∫—É (–≥—Ä–∞–¥–∏–µ–Ω—Ç + —Ç—ë–º–Ω—ã–π –±–ª–æ–∫) —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
‚Äî –ü—É–±–ª–∏–∫—É–µ—Ç –û–¢ –ò–ú–ï–ù–ò –ö–ê–ù–ê–õ–ê (–±–æ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∞–¥–º–∏–Ω–æ–º –∫–∞–Ω–∞–ª–∞)
‚Äî –ü–æ–¥–ø–∏—Å—å: –ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Üí –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ ‚Üí –ò—Å—Ç–æ—á–Ω–∏–∫ (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ) ‚Üí –ò–º—è –∫–∞–Ω–∞–ª–∞ (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ)
"""

import os, re, time, json, html, hashlib, random, datetime
from io import BytesIO
from urllib.parse import urlparse

import requests
import feedparser
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont, ImageFilter

# ---------- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ----------
BOT_TOKEN  = os.getenv("BOT_TOKEN", "").strip()
CHANNEL_ID = os.getenv("CHANNEL_ID", "").strip()   # @usdtdollarm
BRAND      = os.getenv("BRAND", "USDT=Dollar")

DATA_DIR        = "data"
SOURCES_FILE    = os.path.join(DATA_DIR, "sources_ru.txt")
POSTED_FILE     = os.path.join(DATA_DIR, "posted.json")

FRESH_MINUTES   = 120
MIN_CHARS       = 400
CYR_RATIO_MIN   = 0.5
MAX_POSTS_PER_RUN = 1   # –æ–¥–∏–Ω –ø–æ—Å—Ç –∑–∞ –ø—Ä–æ–≥–æ–Ω–∞ ‚Äî –±–µ–∑ —Å–ø–∞–º–∞

IMG_W, IMG_H    = 1024, 512
LOGO_EMOJI      = "üí†"

FONT_REGULAR = os.path.join(DATA_DIR, "DejaVuSans.ttf")
FONT_BOLD    = os.path.join(DATA_DIR, "DejaVuSans-Bold.ttf")

# ---------- –í—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ----------

def tz_msk():
    return datetime.timezone(datetime.timedelta(hours=3))

def now_msk():
    return datetime.datetime.now(tz_msk())

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.replace("www.","")
    except:
        return ""

def ensure_data():
    os.makedirs(DATA_DIR, exist_ok=True)
    if not os.path.exists(POSTED_FILE):
        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump([], f)

def load_posted():
    ensure_data()
    try:
        with open(POSTED_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except:
        return set()

def save_posted(s: set):
    ensure_data()
    with open(POSTED_FILE, "w", encoding="utf-8") as f:
        json.dump(sorted(list(s)), f, ensure_ascii=False)

def read_sources():
    with open(SOURCES_FILE, "r", encoding="utf-8") as f:
        return [l.strip() for l in f if l.strip() and not l.strip().startswith("#")]

def clean_html(txt: str) -> str:
    if not txt: return ""
    txt = html.unescape(txt)
    txt = re.sub(r"<\s*br\s*/?>", "\n", txt, flags=re.I)
    txt = re.sub(r"<[^>]+>", " ", txt)
    txt = re.sub(r"[ \t]+", " ", txt)
    txt = re.sub(r"\s*\n\s*", "\n", txt)
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    txt = re.sub(r"\s+([,.;:!?])", r"\1", txt)
    # —Ä–∞–∑–ª–∏–ø–∞–Ω–∏–µ —Å–ª–µ–ø–ª–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
    txt = re.sub(r"([–∞-—è—ë])([–ê-–Ø–Å])", r"\1 \2", txt)
    return txt.strip()

def cyr_ratio(text: str) -> float:
    if not text: return 0.0
    letters = [ch for ch in text if ch.isalpha()]
    if not letters: return 0.0
    return len([ch for ch in letters if re.match(r"[–ê-–Ø–Å–∞-—è—ë]", ch)]) / len(letters)

def split_sentences(text: str):
    parts = re.split(r"(?<=[.!?])\s+", text.strip())
    return [p.strip() for p in parts if p.strip()]

def drop_noise(text: str) -> str:
    lines = [l.strip() for l in text.splitlines()]
    out = []
    for l in lines:
        if not l:
            out.append(l); continue
        if re.search(r"https?://\S+", l):     # –≥–æ–ª—ã–µ —Å—Å—ã–ª–∫–∏ –≤—ã—á–∏—â–∞–µ–º
            continue
        if re.fullmatch(r"\d{1,2}:\d{2}(\s*\d{2}\.\d{2}\.\d{4})?", l):
            continue
        if len(l) <= 18 and not l.endswith("."):
            continue
        out.append(l)
    cleaned = " ".join(out)
    # —É–±—Ä–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    uniq = []
    seen = set()
    for s in split_sentences(cleaned):
        k = s.lower()
        if k not in seen:
            seen.add(k)
            uniq.append(s)
    return " ".join(uniq)

# ---------- –ú—è–≥–∫–∏–π —Ä–µ—Ä–∞–π—Ç (–ø—Ä–∞–≤–∏–ª–∞) ----------

REWRITE_RULES = [
    # –≥–ª–∞–≥–æ–ª—ã-¬´—Å–æ–æ–±—â–∏–ª/—Å–æ–æ–±—â–∞–µ—Ç¬ª
    (r"\b—Å–æ–æ–±—â–∏–ª(–∞|–∏)?\b", "–∑–∞—è–≤–∏–ª\\1"),
    (r"\b—Å–æ–æ–±—â–∞–µ—Ç\b", "—Å–æ–æ–±—â–∞–µ—Ç, —á—Ç–æ"),
    (r"\b—Å–æ–æ–±—â–∏–ª–∏\b", "–∑–∞—è–≤–∏–ª–∏"),
    (r"\b–ø–æ –¥–∞–Ω–Ω—ã–º\b", "–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"),
    (r"\b—Å–æ–≥–ª–∞—Å–Ω–æ\b", "–∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç –∏–∑"),
    (r"\b–æ—Ç–º–µ—Ç–∏–ª(–∞|–∏)?\b", "–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª\\1"),
    (r"\b–∑–∞—è–≤–∏–ª(–∞|–∏)?\b", "–∑–∞—è–≤–∏–ª\\1"),
    (r"\b—É—Ç–æ—á–Ω–∏–ª(–∞|–∏)?\b", "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–º–µ—Ç–∏–ª\\1"),
    (r"\b–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª(–∞|–∏)?\b", "–æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç–º–µ—Ç–∏–ª\\1"),
    # –≤–≤–æ–¥–Ω—ã–µ
    (r"\b–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏\b", "–Ω–∞–ø—Ä–∏–º–µ—Ä"),
    (r"\b–≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è\b", "–æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"),
    (r"\b–Ω–∞—Ä—è–¥—É —Å —ç—Ç–∏–º\b", "–ø—Ä–∏ —ç—Ç–æ–º"),
    (r"\b–º–µ–∂–¥—É —Ç–µ–º\b", "—Ç–µ–º –≤—Ä–µ–º–µ–Ω–µ–º"),
    # —É—Å—Ç–æ–π—á–∏–≤—ã–µ
    (r"\b–≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è\b", "–≤ –±–ª–∏–∂–∞–π—à–µ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ"),
    (r"\b–≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è\b", "—Å–µ–π—á–∞—Å"),
    (r"\b–≤ —Å–≤—è–∑–∏ —Å\b", "–∏–∑-–∑–∞"),
    (r"\b—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º\b", "–∏—Ç–æ–≥–æ–º —Å—Ç–∞–ª–æ —Ç–æ, —á—Ç–æ"),
    # —Å–∏–Ω—Ç–∞–∫—Å–∏—Å: —á—É—Ç—å –º—è–≥—á–µ
    (r"\s-\s", " ‚Äî "),
]

def soft_rewrite_sentence(s: str) -> str:
    orig = s
    # –Ω–µ —Ç—Ä–æ–≥–∞–µ–º —á–∏—Å–ª–∞, –¥–∞—Ç—ã, –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ ‚Äî –æ–Ω–∏ –æ—Å—Ç–∞–Ω—É—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å
    # –∑–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Å–≤—è–∑–∫–∏/–≤–≤–æ–¥–Ω—ã–µ
    for pat, rep in REWRITE_RULES:
        s = re.sub(pat, rep, s, flags=re.IGNORECASE)
    # –ª—ë–≥–∫–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏: ¬´X, –∑–∞—è–≤–∏–ª(–∞) Y.¬ª ‚Üí ¬´–ö–∞–∫ –∑–∞—è–≤–∏–ª(–∞) Y, X.¬ª
    m = re.search(r"(?P<body>.+?),\s*(–∑–∞—è–≤–∏–ª|–∑–∞—è–≤–∏–ª–∞|–∑–∞—è–≤–∏–ª–∏)\s+(?P<who>[^.]+)\.$", s, flags=re.I)
    if m and len(m.group("body")) > 40:
        s = f"–ö–∞–∫ {m.group(0).split(',')[1].strip()}, {m.group('body')}."
        s = re.sub(r",\s*–∑–∞—è–≤–∏–ª.*", "", s)
    # —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
    if s:
        s = s[:1].upper() + s[1:]
    return s if s.strip() else orig

def rewrite_text(text: str, title: str) -> str:
    sents = split_sentences(text)
    if not sents:
        return ""
    # –∏–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title_norm = title.lower().strip().rstrip(".")
    out = []
    seen = set()
    for i, s in enumerate(sents):
        sn = s.lower().strip()
        if sn.rstrip(".") == title_norm:
            continue
        r = soft_rewrite_sentence(s)
        key = r.lower()
        if key in seen: 
            continue
        seen.add(key)
        out.append(r)
        if len(out) >= 10:
            break
    return " ".join(out).strip()

# ---------- –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π ----------

def fetch_article(url: str) -> str:
    try:
        r = requests.get(url, timeout=12, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        candidates = []
        selectors = [
            "article", ".article__content", "[itemprop='articleBody']",
            ".article", ".news-item__content", ".layout-article",
            ".lenta__text", ".content__body", ".b-material-wrapper"
        ]
        for sel in selectors:
            for el in soup.select(sel):
                t = clean_html(el.get_text("\n"))
                if len(t) > 200:
                    candidates.append(t)
        base = ""
        if candidates:
            base = sorted(candidates, key=len, reverse=True)[0][:8000]
        else:
            base = clean_html(soup.get_text("\n"))[:6000]
        return drop_noise(base)
    except:
        return ""

# ---------- –†–µ–Ω–¥–µ—Ä –æ–±–ª–æ–∂–∫–∏ ----------

def load_font(path, size):
    try:
        return ImageFont.truetype(path, size)
    except:
        return ImageFont.load_default()

def hsv_to_rgb(h,s,v):
    import colorsys
    r,g,b = colorsys.hsv_to_rgb(h/360.0, s, v)
    return (int(r*255), int(g*255), int(b*255))

def wrap_text(draw, text, font, max_width):
    words = text.split()
    lines, line = [], ""
    for w in words:
        t = (line+" "+w).strip()
        bbox = draw.textbbox((0,0), t, font=font)
        if bbox[2]-bbox[0] <= max_width:
            line = t
        else:
            if line: lines.append(line)
            line = w
    if line: lines.append(line)
    return lines

def draw_header(title: str, source: str) -> bytes:
    seed = int(hashlib.sha1(title.encode("utf-8")).hexdigest(), 16)
    random.seed(seed)
    base_h = random.randint(190, 330)      # —Å–ø–æ–∫–æ–π–Ω—ã–µ —Ñ–∏–æ–ª–µ—Ç/—Å–∏–Ω –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
    c1 = hsv_to_rgb(base_h, 0.28, 0.95)
    c2 = hsv_to_rgb((base_h+20)%360, 0.25, 0.78)

    img = Image.new("RGB", (IMG_W, IMG_H), c1)
    grad = Image.new("RGB", (IMG_W, IMG_H), c2)
    mask = Image.linear_gradient("L").resize((IMG_W, IMG_H)).filter(ImageFilter.GaussianBlur(1.5))
    img = Image.composite(grad, img, mask)

    d = ImageDraw.Draw(img)
    # –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–π –ª—ë–≥–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
    for x in range(-IMG_H, IMG_W, 24):
        d.line([(x,0),(x+IMG_H,IMG_H)], fill=(255,255,255,15), width=1)

    # –∑–∞—Ç–µ–º–Ω—ë–Ω–Ω—ã–π –±–ª–æ–∫ –ø–æ–¥ —Ç–µ–∫—Å—Ç
    block = Image.new("RGBA", (IMG_W-80, IMG_H-140), (0,0,0,150))
    img.paste(block, (40,110), block)

    # —à—Ä–∏—Ñ—Ç—ã
    font_title = load_font(FONT_BOLD, 60)
    font_brand = load_font(FONT_BOLD, 32)
    font_tiny  = load_font(FONT_REGULAR, 22)

    # –≤–µ—Ä—Ö–Ω—è—è —Å—Ç—Ä–æ–∫–∞: –ª–æ–≥–æ + –±—Ä–µ–Ω–¥
    d.ellipse((28,18,68,58), fill=(245,245,245))
    d.text((36,24), "$", font=load_font(FONT_BOLD, 24), fill=(20,20,20))
    d.text((78,22), BRAND, font=font_brand, fill=(245,245,245))

    # –∑–∞–≥–æ–ª–æ–≤–æ–∫
    max_w = IMG_W - 140
    y = 150
    for line in wrap_text(d, title, font_title, max_w)[:3]:
        d.text((70,y), line, font=font_title, fill=(245,245,248))
        y += 68

    # –∏—Å—Ç–æ—á–Ω–∏–∫
    d.text((36, IMG_H-40), f"source: {source}", font=font_tiny, fill=(230,230,235))

    out = BytesIO()
    img.save(out, format="JPEG", quality=92)
    return out.getvalue()

# ---------- –¢–µ–ª–µ–≥—Ä–∞–º ----------

def html_escape(s: str) -> str:
    return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def build_caption(title, body, link, channel):
    title_h = f"<b>{html_escape(title.strip())}</b>"
    body = body.strip()
    # –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–µ –∞–±–∑–∞—Ü—ã –ø–æ 2‚Äì4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sents = split_sentences(body)
    paras, cur, lim = [], [], 220
    for s in sents[:10]:
        if len(" ".join(cur+[s])) <= lim:
            cur.append(s)
        else:
            paras.append(" ".join(cur)); cur=[s]
    if cur: paras.append(" ".join(cur))
    details = "\n\n".join(html_escape(p) for p in paras[:3])

    src = domain_of(link)
    source_h = f"<b>–ò—Å—Ç–æ—á–Ω–∏–∫:</b> <a href=\"{html_escape(link)}\">{html_escape(src)}</a>"
    channel_h = f"<a href=\"https://t.me/{channel.lstrip('@')}\">{html_escape(channel.lstrip('@'))}</a>"

    parts = [title_h, "", details, "", source_h, channel_h]
    cap = "\n".join([p for p in parts if p is not None]).strip()
    if len(cap) > 1024:
        # –º—è–≥–∫–æ —É–∫–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏
        keep = 1024 - (len(cap) - len(details)) - 10
        details = html_escape(details[:keep].rsplit(" ",1)[0]) + "‚Ä¶"
        parts[2] = details
        cap = "\n".join(parts).strip()
    return cap

def send_photo(token, chat_id, photo_bytes, caption_html):
    url = f"https://api.telegram.org/bot{token}/sendPhoto"
    files = {"photo": ("header.jpg", photo_bytes, "image/jpeg")}
    data = {"chat_id": chat_id, "caption": caption_html, "parse_mode": "HTML", "disable_web_page_preview": True}
    r = requests.post(url, data=data, files=files, timeout=20)
    if r.status_code != 200:
        raise RuntimeError(f"sendPhoto {r.status_code}: {r.text}")

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ----------

def gather_items():
    items = []
    for feed in read_sources():
        try:
            fp = feedparser.parse(feed)
            for e in fp.entries[:12]:
                link = e.get("link") or e.get("id") or ""
                title = clean_html(e.get("title",""))
                if not link or not title: 
                    continue
                pp = e.get("published_parsed") or e.get("updated_parsed")
                if pp:
                    dt = datetime.datetime.fromtimestamp(
                        time.mktime(pp),
                        datetime.timezone.utc
                    ).astimezone(tz_msk())
                else:
                    dt = now_msk()
                items.append((dt, title, link))
        except Exception:
            continue
    items.sort(key=lambda x: x[0], reverse=True)
    return items

def main():
    assert BOT_TOKEN and CHANNEL_ID, "–ó–∞–¥–∞–π BOT_TOKEN –∏ CHANNEL_ID –≤ Secrets."
    posted = load_posted()
    posted_changed = False
    posted_count = 0

    for dt, title, link in gather_items():
        if posted_count >= MAX_POSTS_PER_RUN:
            break
        if link in posted:
            continue
        if (now_msk()-dt).total_seconds() > FRESH_MINUTES*60:
            continue

        raw = fetch_article(link)
        text = clean_html(raw)
        text = drop_noise(text)

        if len(text) < MIN_CHARS or cyr_ratio(text) < CYR_RATIO_MIN:
            continue

        # —Ä–µ—Ä–∞–π—Ç ‚âà50%
        body = rewrite_text(text, title)
        if len(body) < MIN_CHARS:
            # –µ—Å–ª–∏ —Å–∏–ª—å–Ω–æ —É–∂–∞–ª–æ—Å—å ‚Äî –±–µ—Ä—ë–º —á–∞—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ + —Ä–µ—Ä–∞–π—Ç –æ—Å—Ç–∞–≤—à–µ–π—Å—è
            body = rewrite_text(text[:2000], title)

        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_norm = title.lower().strip().rstrip(".")
        body_sents = [s for s in split_sentences(body) if s.lower().strip().rstrip(".") != title_norm]
        body = " ".join(body_sents).strip()

        if len(body) < MIN_CHARS:
            continue

        # –∫–∞—Ä—Ç–∏–Ω–∫–∞ + –ø–æ–¥–ø–∏—Å—å
        photo = draw_header(title, domain_of(link))
        caption = build_caption(title, body, link, CHANNEL_ID)

        try:
            send_photo(BOT_TOKEN, CHANNEL_ID, photo, caption)
            posted.add(link); posted_changed = True; posted_count += 1
            print("Posted:", title)
        except Exception as ex:
            print("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏:", ex)
            continue

    if posted_changed:
        save_posted(posted)
    if posted_count == 0:
        print("–ü–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

if __name__ == "__main__":
    main()
