# -*- coding: utf-8 -*-
"""
USDT=Dollar ‚Äî –∞–≤—Ç–æ-–ø–æ—Å—Ç–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π (RU)
- RU-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ RSS
- –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏, –º—è–≥–∫–∏–π —Ä–µ—Ä–∞–π—Ç –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤
- –æ—Ç—Ä–∏—Å–æ–≤–∫–∞ —à–∞–ø–∫–∏ (–≥—Ä–∞–¥–∏–µ–Ω—Ç + –ª–æ–≥–æ + —Ñ—É—Ç–µ—Ä)
- HTML-–∫–∞–ø—à–µ–Ω —Å —Ç–∞–π—Ç–ª–æ–º, –ª–∏–¥–æ–º, –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏, –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –∏ —Ç–µ–≥–∞–º–∏ (–≤ —Å–ø–æ–π–ª–µ—Ä–µ)
- –∞–Ω—Ç–∏–¥—É–±–ª—å, —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞, –∫–∏—Ä–∏–ª–ª–∏—Ü–∞, –¥–ª–∏–Ω–Ω–∞
"""

import os, re, json, time, html, math, hashlib, random, textwrap, datetime
from urllib.parse import urlparse
import requests
import feedparser

from bs4 import BeautifulSoup  # bs4 —É–∫–∞–∑–∞–Ω –≤ requirements
from PIL import Image, ImageDraw, ImageFont, ImageFilter

# -------------------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è --------------------

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ)
FEEDS = [
    "https://www.rbc.ru/rss/?rss=news",
    "https://lenta.ru/rss",
    "https://www.kommersant.ru/RSS/news.xml",
    "https://www.gazeta.ru/export/rss/first.xml",
    "https://www.interfax.ru/rss.asp",
    "https://iz.ru/xml/rss/all.xml",
    "https://ria.ru/export/rss2/archive/index.xml",
    "https://www.vedomosti.ru/rss/news",
]

# –°–µ–∫—Ä–µ—Ç—ã
BOT_TOKEN  = os.getenv("BOT_TOKEN", "").strip()
CHANNEL_ID = os.getenv("CHANNEL_ID", "").strip()  # –ø—Ä–∏–º–µ—Ä: @usdtdollarm

# –õ–∏–º–∏—Ç—ã –∏ —Ñ–∏–ª—å—Ç—Ä—ã
MIN_BODY_CHARS     = 400        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ—Å—Ç–∞
CYR_RATIO_MIN      = 0.5        # –¥–æ–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –≤ —Ç–µ–∫—Å—Ç–µ
FRESH_MINUTES      = 90         # –æ–∫–Ω–æ —Å–≤–µ–∂–µ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏
POSTED_PATH        = "data/posted.json"

# –®—Ä–∏—Ñ—Ç—ã
FONT_REGULAR_PATH  = "data/DejaVuSans.ttf"
FONT_BOLD_PATH     = "data/DejaVuSans-Bold.ttf"

# –ö–∞–Ω–∞–ª—å–Ω—ã–π –±—Ä–µ–Ω–¥–∏–Ω–≥
CHANNEL_NAME_SHORT = "USDT=Dollar"   # –ø–æ–¥–ø–∏—Å—å –≤ —à–∞–ø–∫–µ
LOGO_EMOJI         = "üí†"            # –∫—Ä—É–∂–æ–∫-–ª–æ–≥–æ—Ç–∏–ø (—Ä–∏—Å—É–µ–º –≤ —à–∞–ø–∫–µ)
IMG_W, IMG_H       = 1024, 512       # —à–∞–ø–∫–∞

# -------------------- –£—Ç–∏–ª–∏—Ç—ã --------------------

def now_msk():
    tz = datetime.timezone(datetime.timedelta(hours=3))
    return datetime.datetime.now(tz)

def load_posted():
    if not os.path.exists(POSTED_PATH):
        return set()
    try:
        with open(POSTED_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(data if isinstance(data, list) else [])
    except:
        return set()

def save_posted(s: set):
    os.makedirs(os.path.dirname(POSTED_PATH), exist_ok=True)
    with open(POSTED_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(list(s)), f, ensure_ascii=False, indent=2)

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.replace("www.","")
    except:
        return ""

def clean_html(txt: str) -> str:
    if not txt: return ""
    # HTML -> —Ç–µ–∫—Å—Ç
    txt = html.unescape(txt)
    # –£–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏
    txt = re.sub(r"<\s*br\s*/?>", "\n", txt, flags=re.I)
    txt = re.sub(r"<[^>]+>", " ", txt)
    # –ú–Ω–æ–≥–æ–ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–Ω–æ—Å—ã
    txt = re.sub(r"[ \t]+", " ", txt)
    txt = re.sub(r"\s*\n\s*", "\n", txt)
    txt = re.sub(r"\n{3,}", "\n\n", txt)
    # –ü—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π
    txt = re.sub(r"\s+([,.;:!?])", r"\1", txt)
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏—Ä–µ, –∫–∞–≤—ã—á–µ–∫
    txt = txt.replace(" - ", " ‚Äî ")
    txt = txt.replace("\"", "¬´").replace("¬´¬´", "¬´").replace("¬ª¬ª", "¬ª").replace("¬ª¬´", "¬ª ¬´")
    # –°–ª–∏—Ç–Ω–æ—Å—Ç–∏ —Ç–∏–ø–∞ ¬´—É—á–∞—Å—Ç–Ω–∏–∫–∏–æ–ø—Ä–æ—Å–∞¬ª
    txt = re.sub(r"([–∞-—è—ë])([–ê-–Ø–Å])", r"\1 \2", txt)
    return txt.strip()

def cyr_ratio(txt: str) -> float:
    if not txt: return 0.0
    total = len([ch for ch in txt if ch.isalpha()])
    if total == 0: return 0.0
    cyr = len(re.findall(r"[–∞-—è—ë–ê-–Ø–Å]", txt))
    return cyr / total

def soft_rewrite(sent: str) -> str:
    """–û—á–µ–Ω—å –º—è–≥–∫–∏–π —Ä–µ—Ä–∞–π—Ç (–∑–∞–º–µ–Ω—ã –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤, –≤–≤–æ–¥–Ω—ã—Ö, –ø–æ—Ä—è–¥–æ–∫ —á–∞—Å—Ç–µ–π).
       –ù–∏–∫–∞–∫–∏—Ö –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤/—á–∏—Å–µ–ª/–∏–º–µ–Ω!"""
    if not sent: return ""
    s = sent

    # –í–≤–æ–¥–Ω—ã–µ
    repls = {
        "—Å–æ–æ–±—â–∏–ª ": "–∑–∞—è–≤–∏–ª ",
        "—Å–æ–æ–±—â–∏–ª–∞ ": "–∑–∞—è–≤–∏–ª–∞ ",
        "—Å–æ–æ–±—â–∏–ª–∏ ": "–∑–∞—è–≤–∏–ª–∏ ",
        "—Å–æ–æ–±—â–∞–µ—Ç—Å—è, —á—Ç–æ": "—É—Ç–æ—á–Ω—è–µ—Ç—Å—è, —á—Ç–æ",
        "–æ—Ç–º–µ—Ç–∏–ª ": "–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª ",
        "–æ—Ç–º–µ—Ç–∏–ª–∞ ": "–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª–∞ ",
        "–æ—Ç–º–µ—Ç–∏–ª–∏ ": "–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª–∏ ",
        "–ø–æ –¥–∞–Ω–Ω—ã–º ": "–ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ",
        "–≤ —Å–≤–æ–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏": "–≤ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
        "–≤ —Å–≤–æ–µ–º –∑–∞—è–≤–ª–µ–Ω–∏–∏": "–≤ –∑–∞—è–≤–ª–µ–Ω–∏–∏",
    }
    for k,v in repls.items():
        s = re.sub(r"\b"+re.escape(k)+r"\b", v, s, flags=re.IGNORECASE)

    # –õ—ë–≥–∫–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞: ¬´X, —Å–æ–æ–±—â–∏–ª Y.¬ª ‚Üí ¬´–ö–∞–∫ –∑–∞—è–≤–∏–ª Y, X.¬ª
    m = re.search(r"(?P<body>.+?),\s*(—Å–æ–æ–±—â–∏–ª|—Å–æ–æ–±—â–∏–ª–∞|—Å–æ–æ–±—â–∏–ª–∏|–∑–∞—è–≤–∏–ª|–∑–∞—è–≤–∏–ª–∞|–∑–∞—è–≤–∏–ª–∏)\s+(?P<who>[^.]+)\.$", s, flags=re.I)
    if m and len(m.group("body"))>40:
        verb = "–ö–∞–∫ " + ("–∑–∞—è–≤–∏–ª" if re.search(r"(–∑–∞—è–≤–∏–ª|—Å–æ–æ–±—â–∏–ª)", m.group(0), re.I) else "–∑–∞—è–≤–∏–ª–∏")
        s = f"{verb} {m.group('who')}, {m.group('body')}."

    # –ö–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    s = s[:1].upper() + s[1:] if s else s
    return s

def split_sentences(text: str) -> list:
    # –ø—Ä–æ—Å—Ç–∞—è —Ä–∞–∑—Ä–µ–∑–∫–∞ (–±–µ–∑ —Ç—è–∂–µ–ª–æ–≥–æ nltk)
    parts = re.split(r"(?<=[.!?])\s+", text.strip())
    parts = [p.strip() for p in parts if p.strip()]
    return parts

def pick_lead_and_body(full: str) -> tuple[str, str]:
    sents = split_sentences(full)
    if not sents:
        return "", ""
    lead = sents[0]
    body = " ".join(sents[1:]) if len(sents) > 1 else ""
    # –º—è–≥–∫–∏–π —Ä–µ—Ä–∞–π—Ç
    lead = soft_rewrite(lead)
    body_sents = [soft_rewrite(x) for x in split_sentences(body)]
    # —É–±—Ä–∞—Ç—å –ø—Ä—è–º—ã–µ –¥—É–±–ª–∏
    body_sents = [x for x in body_sents if x.lower() != lead.lower()]
    body = " ".join(body_sents)
    return lead, body

def keywords_to_tags(title: str, body: str, k: int = 5) -> list:
    text = (title + " " + body).lower()
    # —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + –¥–µ—Ñ–∏—Å
    words = re.findall(r"[–∞-—è—ë\-]{4,}", text)
    stop = set("—ç—Ç–æ —Ç–∞–∫–æ–π —Ç–∞–∫–∂–µ —Ç–∞–∫–∂–µ-—Ç–æ –º–æ–∂–µ—Ç —á—Ç–æ–±—ã –ø–æ—Å–ª–µ –ø–µ—Ä–µ–¥ –º–µ–∂–¥—É —á–µ—Ä–µ–∑ –≤—Å–µ–≥–æ –±–æ–ª–µ–µ –æ–∫–æ–ª–æ —Ç–æ–≥–¥–∞ –æ—á–µ–Ω—å –±—ã—Ç—å –±—ã–ª–∏ –±—ã–ª–æ –±—ã–ª–æ –±—ã –ª–∏–±–æ —Ç–∏–ø–∞ –≤—Ä–æ–¥–µ –ª–∏—à—å —É–∂–µ –µ—â—ë –µ—â–µ –∏–ª–∏ –ø—Ä–∏ –±–µ–∑ –¥–ª—è –Ω–∞–¥ –ø–æ–¥ –ø—Ä–æ –∫–∞–∫ —á–µ–º —á–µ–º-—Ç–æ —á–µ–º-–ª–∏–±–æ —á—Ç–æ —á—Ç–æ–±—ã –∫–æ–≥–æ —á–µ–≥–æ –∫—É–¥–∞ –∫–æ–≥–¥–∞ –≥–¥–µ –∫–∞–∫–∞—è –∫–∞–∫–∏–µ –∫–∞–∫–∏—Ö –∫–∞–∫–∏—Ö-—Ç–æ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –ø–æ—á–µ–º—É –∑–∞—Ç–æ –∑–∞—Ç–æ-—Ç–æ –ª–∏–±–æ-—Ç–æ".split())
    freq = {}
    for w in words:
        if w in stop: continue
        freq[w] = freq.get(w, 0) + 1
    # —Ç–æ–ø –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∏ –¥–ª–∏–Ω–µ
    cand = sorted(freq.items(), key=lambda x: (x[1], len(x[0])), reverse=True)
    tags = []
    for w,_ in cand:
        if len(tags) >= k: break
        if all(w not in t and t not in w for t in tags):
            tags.append("#"+w.replace("‚Äî","-").replace("‚Äì","-"))
    return tags

def draw_header(title: str, source_domain: str, event_dt: datetime.datetime) -> bytes:
    """–®–∞–ø–∫–∞: –≥—Ä–∞–¥–∏–µ–Ω—Ç + —Å–∫–æ—à–µ–Ω–Ω–∞—è –º–∞—Å–∫–∞ + –ª–æ–≥–æ—Ç–∏–ø + —Ñ—É—Ç–µ—Ä + –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏"""
    # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ —Ö—ç—à—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
    h = int(hashlib.md5(title.encode("utf-8")).hexdigest(), 16)
    random.seed(h)
    def rnd(): return random.randint(90, 200)
    c1 = (rnd(), rnd(), 255 - rnd()//2)
    c2 = (255 - rnd()//3, rnd(), rnd())

    img = Image.new("RGB", (IMG_W, IMG_H), c1)
    grad = Image.new("RGB", (IMG_W, IMG_H), c2)
    mask = Image.linear_gradient("L").resize((IMG_W, IMG_H)).filter(ImageFilter.GaussianBlur(2))
    img = Image.composite(grad, img, mask)

    # –°–∫–æ—à–µ–Ω–Ω–∞—è –ø–ª–∞—à–∫–∞
    d = ImageDraw.Draw(img)
    angle_h = IMG_H//5
    d.polygon([(0,0),(IMG_W,0),(IMG_W,angle_h),(0,angle_h+40)], fill=(0,0,0,70))

    # –®—Ä–∏—Ñ—Ç—ã
    def load_font(path, size):
        try:
            return ImageFont.truetype(path, size)
        except:
            return ImageFont.load_default()
    font_b = load_font(FONT_BOLD_PATH, 46)
    font_r = load_font(FONT_REGULAR_PATH, 24)
    font_m = load_font(FONT_BOLD_PATH, 28)

    # –õ–æ–≥–æ—Ç–∏–ø
    d.ellipse((24,24,72,72), fill=(255,255,255,220))
    d.text((34,30), LOGO_EMOJI, font=load_font(FONT_REGULAR_PATH, 28), fill=(30,30,30))
    d.text((90,38), CHANNEL_NAME_SHORT, font=font_m, fill=(240,240,245))

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –ø–µ—Ä–µ–Ω–æ—Å—ã
    max_w = IMG_W - 120
    lines = []
    for para in title.split("\n"):
        acc = ""
        for w in para.split():
            t = (acc + " " + w).strip()
            bbox = d.textbbox((0,0), t, font=font_b)
            if bbox[2] - bbox[0] <= max_w:
                acc = t
            else:
                if acc: lines.append(acc)
                acc = w
        if acc: lines.append(acc)
    # –†–∏—Å—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    y = 170
    for line in lines[:4]:
        d.text((60,y), line, font=font_b, fill=(245,245,248))
        y += 56

    # –§—É—Ç–µ—Ä
    foot = f"source: {source_domain}  ‚Ä¢  —Å–æ–±—ã—Ç–∏–µ: {event_dt.strftime('%d.%m %H:%M')}"
    d.text((60, IMG_H-42), foot, font=font_r, fill=(230,230,235))

    # –í –±–∞–π—Ç—ã
    out = requests.compat.BytesIO()
    img.save(out, format="JPEG", quality=92)
    return out.getvalue()

def fetch_article(url: str) -> str:
    """–¢—è–Ω–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø—Ä–æ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç."""
    try:
        r = requests.get(url, timeout=10, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        # –ø—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ –±–ª–æ–∫–∞–º
        cand = []
        for sel in ["article","div[itemprop='articleBody']",".article__content",".layout-article",".news-body",".article","main",".lenta__text","[class*=content]"]:
            for el in soup.select(sel):
                txt = clean_html(el.get_text("\n"))
                if len(txt) > 200:
                    cand.append(txt)
        if not cand:
            txt = clean_html(soup.get_text("\n"))
            return txt[:4000]
        # —Å–∞–º–∞—è –¥–ª–∏–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
        return sorted(cand, key=len, reverse=True)[0][:6000]
    except:
        return ""

def html_escape(s: str) -> str:
    return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def build_caption(title: str, lead: str, body: str, source_url: str, tags: list) -> str:
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–∂–∏—Ä–Ω—ã–π), –ª–∏–¥ –∏ –¥–µ—Ç–∞–ª–∏ (–∫–∞–∂–¥—ã–π —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
    def cap(s): 
        s = s.strip()
        return s[:1].upper()+s[1:] if s else s
    title_h = f"<b>{html_escape(cap(title))}</b>"
    lead_h  = f"üì∞ {html_escape(cap(lead))}"
    body_h  = html_escape(cap(body))

    # –ò—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞
    src_h   = f"<b>–ò—Å—Ç–æ—á–Ω–∏–∫:</b> <a href=\"{html_escape(source_url)}\">{html_escape(domain_of(source_url))}</a>"

    # –¢–µ–≥–∏ ‚Äî —Å–ø–æ–π–ª–µ—Ä—ã (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—è–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è)
    if tags:
        tags_sp = " ".join([f"<span class=\"tg-spoiler\">{html_escape(t)}</span>" for t in tags[:5]])
        tags_h = f"\n\n{tags_sp}"
    else:
        tags_h = ""

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–ø—à–µ–Ω
    parts = [
        title_h, "",  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        lead_h,
        "", "<b>–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏:</b>",
        body_h,
        "", src_h,
        tags_h
    ]
    res = "\n".join([p for p in parts if p is not None])
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram ~1024 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è caption —Ñ–æ—Ç–æ ‚Äî —Å—Ç—Ä–µ–º–∏–º—Å—è —É–ª–æ–∂–∏—Ç—å—Å—è
    if len(res) > 1024:
        # —Å–æ–∫—Ä–∞—â–∞–µ–º body
        cut = 1024 - (len(res) - len(body_h)) - 20
        body_h = html_escape(body_h[:max(0,cut)].rsplit(" ",1)[0]) + "‚Ä¶"
        parts[5] = body_h
        res = "\n".join([p for p in parts if p is not None])
    return res

def send_photo(token: str, chat_id: str, photo_bytes: bytes, caption_html: str):
    url = f"https://api.telegram.org/bot{token}/sendPhoto"
    files = {"photo": ("header.jpg", photo_bytes, "image/jpeg")}
    data = {"chat_id": chat_id, "caption": caption_html, "parse_mode": "HTML", "disable_web_page_preview": True}
    r = requests.post(url, data=data, files=files, timeout=20)
    if r.status_code != 200:
        raise RuntimeError(f"Telegram sendPhoto: {r.status_code} {r.text}")

# -------------------- –ì–ª–∞–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ --------------------

def pick_best_item():
    """–°–æ–±–∏—Ä–∞–µ–º –ª–µ–Ω—Ç—ã, –≤—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –Ω–æ–≤–æ—Å—Ç—å."""
    items = []
    for feed in FEEDS:
        try:
            fp = feedparser.parse(feed)
            for e in fp.entries[:12]:
                link = e.get("link") or e.get("id") or ""
                title = clean_html(e.get("title",""))
                if not link or not title: 
                    continue
                # –≤—Ä–µ–º—è
                published_parsed = e.get("published_parsed") or e.get("updated_parsed")
                if published_parsed:
                    dt = datetime.datetime.fromtimestamp(time.mktime(published_parsed), datetime.timezone.utc).astimezone(datetime.timezone(datetime.timedelta(hours=3)))
                else:
                    dt = now_msk()
                items.append((dt, title, link))
        except:
            continue
    # –°–≤–µ–∂–µ–µ —Å–Ω–∞—á–∞–ª–∞
    items.sort(key=lambda x: x[0], reverse=True)
    return items

def main():
    assert BOT_TOKEN and CHANNEL_ID, "–ù–µ –∑–∞–¥–∞–Ω—ã BOT_TOKEN / CHANNEL_ID (Secrets –≤ GitHub)"
    posted = load_posted()

    for dt, title, link in pick_best_item():
        if link in posted:
            continue
        # –æ–∫–Ω–æ —Å–≤–µ–∂–µ—Å—Ç–∏
        if (now_msk() - dt).total_seconds() > FRESH_MINUTES*60:
            continue

        # –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        raw = fetch_article(link)
        text = clean_html(raw)
        # –ë—ã—Å—Ç—Ä–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç ¬´–ø—É—Å—Ç—ã—Ö¬ª/–æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        if len(text) < MIN_BODY_CHARS or cyr_ratio(text) < CYR_RATIO_MIN:
            continue

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–∂–µ —á–∏—Å—Ç–∏–º
        title = clean_html(title)

        # –õ–∏–¥ + –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
        lead, body = pick_lead_and_body(text)
        if not lead or not body or (lead.lower() in body.lower()):
            # –µ—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ ¬´—Å—É—Ö–æ¬ª ‚Äî –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å 2‚Äì3 –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ body
            sents = split_sentences(text)
            if len(sents) >= 3:
                lead = soft_rewrite(sents[0])
                body = " ".join([soft_rewrite(x) for x in sents[1:4]])
            else:
                continue

        # –¢–µ–≥–∏
        tags = keywords_to_tags(title, body, k=5)

        # –®–∞–ø–∫–∞
        photo = draw_header(title, domain_of(link), dt)

        # –ö–∞–ø—à–µ–Ω
        caption = build_caption(title, lead, body, link, tags)

        # –û—Ç–ø—Ä–∞–≤–∫–∞
        try:
            send_photo(BOT_TOKEN, CHANNEL_ID, photo, caption)
            posted.add(link)
            save_posted(posted)
            print(f"Posted: {title}")
            return  # –ø—É–±–ª–∏–∫—É–µ–º –æ–¥–Ω—É –Ω–æ–≤–æ—Å—Ç—å –∑–∞ –∑–∞–ø—É—Å–∫
        except Exception as ex:
            print("–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:", ex)
            # –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é
            continue

    print("–ü–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫.")

if __name__ == "__main__":
    main()
