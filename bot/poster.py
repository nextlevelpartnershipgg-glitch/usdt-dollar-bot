import os, io, json, time, textwrap, pathlib, hashlib, urllib.parse, random, re, math
from datetime import datetime, timezone, timedelta
from dateutil import parser as dtparse
import feedparser, requests
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
from zoneinfo import ZoneInfo

# ========= –ë–ê–ó–û–í–´–ï –ù–ê–°–¢–†–û–ô–ö–ò =========
BOT_TOKEN  = os.environ.get("BOT_TOKEN")
CHANNEL_ID = os.environ.get("CHANNEL_ID", "@usdtdollarm")
TIMEZONE   = os.environ.get("TIMEZONE", "Europe/Zurich")

CHANNEL_NAME   = os.environ.get("CHANNEL_NAME", "USDT=Dollar")
CHANNEL_HANDLE = os.environ.get("CHANNEL_HANDLE", "@usdtdollarm")
CHANNEL_LINK   = os.environ.get("CHANNEL_LINK", f"https://t.me/{CHANNEL_HANDLE.lstrip('@')}")

MAX_POSTS_PER_RUN = int(os.environ.get("MAX_POSTS_PER_RUN", "5"))
LOOKBACK_MINUTES  = int(os.environ.get("LOOKBACK_MINUTES", "90"))

# ========= RSS –ò–°–¢–û–ß–ù–ò–ö–ò =========
RSS_FEEDS_RU = [
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "https://rssexport.rbc.ru/rbcnews/economics/30/full.rss",
    "https://rssexport.rbc.ru/rbcnews/finance/30/full.rss",
    "https://rssexport.rbc.ru/rbcnews/politics/30/full.rss",
    "https://lenta.ru/rss/news",
    "https://lenta.ru/rss/economics",
    "https://lenta.ru/rss/russia",
    "https://lenta.ru/rss/world",
    "https://www.kommersant.ru/RSS/news.xml",
    "https://www.kommersant.ru/RSS/economics.xml",
    "https://www.kommersant.ru/RSS/finance.xml",
    "https://www.gazeta.ru/export/rss/first.xml",
    "https://www.gazeta.ru/export/rss/business.xml",
    "https://www.gazeta.ru/export/rss/politics.xml",
    "https://tass.ru/rss/v2.xml",
    "https://www.vedomosti.ru/rss/news",
    "https://www.interfax.ru/rss.asp",
    "https://ria.ru/export/rss2/archive/index.xml",
    "https://iz.ru/xml/rss/all.xml",
    "https://www.finmarket.ru/rss/news.asp",
    "https://banki.ru/xml/news.rss",
    "https://www.kommersant.ru/RSS/regions.xml",
    "https://www.kommersant.ru/RSS/tech.xml",
]
RSS_FEEDS_WORLD = [
    "https://feeds.reuters.com/reuters/businessNews",
    "https://feeds.reuters.com/Reuters/worldNews",
    "https://feeds.reuters.com/reuters/marketsNews",
    "https://feeds.bloomberg.com/politics/news.rss",
    "https://www.bloomberg.com/feeds/podcasts/etf_report.xml",
    "https://www.ft.com/?format=rss",
    "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
    "https://feeds.a.dj.com/rss/RSSWorldNews.xml",
    "http://feeds.bbci.co.uk/news/world/rss.xml",
    "http://feeds.bbci.co.uk/news/business/rss.xml",
    "http://rss.cnn.com/rss/edition_world.rss",
    "https://www.cnbc.com/id/100003114/device/rss/rss.html",
    "https://www.theguardian.com/world/rss",
    "https://www.theguardian.com/uk/business/rss",
    "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml",
    "https://www.marketwatch.com/rss/topstories",
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://asia.nikkei.com/rss",
    "https://www.scmp.com/rss/91/feed",
    "https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml",
    "https://cointelegraph.com/rss",
]
RSS_FEEDS = RSS_FEEDS_RU + RSS_FEEDS_WORLD

DATA_DIR = pathlib.Path("data"); DATA_DIR.mkdir(parents=True, exist_ok=True)
STATE_FILE = DATA_DIR / "state.json"

UA  = {"User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
UA_IMG = {"User-Agent":"Mozilla/5.0"}

# ========= PYMORPHY2 (–¥–ª—è —Ç–µ–≥–æ–≤) =========
try:
    import pymorphy2
    MORPH = pymorphy2.MorphAnalyzer()
except Exception:
    MORPH = None  # –º—è–≥–∫–∏–π —Ñ–æ–ª–±—ç–∫

# ========= –£–¢–ò–õ–ò–¢–´ =========
def load_state():
    if STATE_FILE.exists():
        return json.loads(STATE_FILE.read_text(encoding="utf-8"))
    return {}

def save_state(state):
    STATE_FILE.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def domain(url):
    return urllib.parse.urlparse(url).netloc.replace("www.", "") or "source"

def root_domain(url):
    try:
        dom = urllib.parse.urlparse(url).netloc.replace("www.","")
        parts = dom.split(".")
        if len(parts) > 2:
            dom = ".".join(parts[-2:])
        return dom
    except Exception:
        return "–∏—Å—Ç–æ—á–Ω–∏–∫"

def clean_html(html):
    if not html:
        return ""
    soup = BeautifulSoup(html, "html.parser")
    return " ".join(soup.get_text(separator=" ").split())

def clamp(s, n):
    s = (s or "").strip()
    return s if len(s) <= n else s[:n-1] + "‚Ä¶"

# ========= –Ø–ó–´–ö / –ü–ï–†–ï–í–û–î =========
def detect_lang(text: str) -> str:
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", text):
        return "ru"
    en_hits = len(re.findall(r"\b(the|and|of|to|in|for|on|with|from|by|as|at|is|are)\b", text.lower()))
    ru_hits = len(re.findall(r"\b(–∏|–≤|–Ω–∞|–ø–æ|–¥–ª—è|–∏–∑|–æ—Ç|–∫–∞–∫|—ç—Ç–æ|—á—Ç–æ|–±—ã|–Ω–µ|–∫)\b", text.lower()))
    return "en" if en_hits > ru_hits else "ru"

LT_ENDPOINTS = [
    "https://libretranslate.de/translate",
    "https://translate.argosopentech.com/translate",
]
LOCAL_EN_RU = {
    "fed": "–§–†–°", "ecb":"–ï–¶–ë", "bank of england":"–ë–∞–Ω–∫ –ê–Ω–≥–ª–∏–∏", "bank of japan":"–ë–∞–Ω–∫ –Ø–ø–æ–Ω–∏–∏",
    "inflation":"–∏–Ω—Ñ–ª—è—Ü–∏—è", "cpi":"–∏–Ω–¥–µ–∫—Å CPI", "ppi":"–∏–Ω–¥–µ–∫—Å PPI",
    "rate":"—Å—Ç–∞–≤–∫–∞", "rates":"—Å—Ç–∞–≤–∫–∏", "hike":"–ø–æ–≤—ã—à–µ–Ω–∏–µ", "cut":"—Å–Ω–∏–∂–µ–Ω–∏–µ",
    "recession":"—Ä–µ—Ü–µ—Å—Å–∏—è", "growth":"—Ä–æ—Å—Ç", "gdp":"–í–í–ü",
    "oil":"–Ω–µ—Ñ—Ç—å", "gas":"–≥–∞–∑", "brent":"Brent", "wti":"WTI",
    "stocks":"–∞–∫—Ü–∏–∏", "bonds":"–æ–±–ª–∏–≥–∞—Ü–∏–∏", "equities":"–∞–∫—Ü–∏–∏", "yields":"–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏",
    "dollar":"–¥–æ–ª–ª–∞—Ä", "euro":"–µ–≤—Ä–æ", "ruble":"—Ä—É–±–ª—å", "yuan":"—é–∞–Ω—å",
    "bitcoin":"–±–∏—Ç–∫–æ–∏–Ω", "ethereum":"—ç—Ñ–∏—Ä–∏—É–º", "crypto":"–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞",
}
def translate_en_to_ru(text: str, timeout=12) -> str:
    text = text.strip()
    if not text:
        return text
    for ep in LT_ENDPOINTS:
        try:
            r = requests.post(ep, data={"q": text, "source":"en", "target":"ru", "format":"text"},
                              headers={"Accept":"application/json"}, timeout=timeout)
            if r.status_code == 200:
                out = (r.json() or {}).get("translatedText", "")
                if out.strip():
                    return out.strip()
        except Exception:
            continue
    s = text
    for k in sorted(LOCAL_EN_RU.keys(), key=lambda x: -len(x)):
        s = re.sub(rf"\b{re.escape(k)}\b", LOCAL_EN_RU[k], s, flags=re.IGNORECASE)
    return s

# ========= –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–£–©–ù–û–°–¢–ï–ô =========
COMPANY_HINTS = [
    "Apple","Microsoft","Tesla","Meta","Google","Alphabet","Amazon","Nvidia","Samsung","Intel","Huawei",
    "–ì–∞–∑–ø—Ä–æ–º","–°–±–µ—Ä–±–∞–Ω–∫","–Ø–Ω–¥–µ–∫—Å","–†–æ—Å–Ω–µ—Ñ—Ç—å","–õ—É–∫–æ–π–ª","–ù–æ—Ä–Ω–∏–∫–µ–ª—å","–¢–∞—Ç–Ω–µ—Ñ—Ç—å","–ù–æ–≤–∞—Ç—ç–∫","–í–¢–ë","–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑"
]
TICKER_PAT = re.compile(r"\b[A-Z]{2,6}\b")

def extract_entities(title, summary):
    text = f"{title} {summary}".strip()
    names = re.findall(r"(?:[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+(?:\s+[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+){0,2})", text)
    tickers = [m for m in TICKER_PAT.findall(text) if m not in ("NEWS","HTTP","HTTPS","HTML")]
    companies = [c for c in COMPANY_HINTS if c.lower() in text.lower()]
    stop = {"The","This"}
    names = [x for x in names if x not in stop and len(x) > 2]
    out = []
    out += names[:5]; out += companies[:5]; out += tickers[:5]
    seen=set(); uniq=[]
    for x in out:
        if x not in seen:
            seen.add(x); uniq.append(x)
    return uniq or ["finance","market"]

# ========= –ö–û–ù–¢–ï–ö–°–¢–ù–û–ï –§–û–¢–û: Wikipedia ‚Üí Unsplash ‚Üí Picsum ‚Üí –ì—Ä–∞–¥–∏–µ–Ω—Ç =========
COUNTRY_PROPER = {
    "—Ä–æ—Å—Å–∏—è":"–†–æ—Å—Å–∏—è","—Å—à–∞":"–°–®–ê","–∫–∏—Ç–∞–π":"–ö–∏—Ç–∞–π","—è–ø–æ–Ω–∏—è":"–Ø–ø–æ–Ω–∏—è","–≥–µ—Ä–º–∞–Ω–∏—è":"–ì–µ—Ä–º–∞–Ω–∏—è","—Ñ—Ä–∞–Ω—Ü–∏—è":"–§—Ä–∞–Ω—Ü–∏—è",
    "–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è":"–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è","–∏–Ω–¥–∏—è":"–ò–Ω–¥–∏—è","–µ–≤—Ä–æ–ø–∞":"–ï–≤—Ä–æ–ø–∞","—É–∫—Ä–∞–∏–Ω–∞":"–£–∫—Ä–∞–∏–Ω–∞","—Ç—É—Ä—Ü–∏—è":"–¢—É—Ä—Ü–∏—è",
}
def topic_wiki_titles(text_lower):
    t = text_lower; titles=[]
    a=titles.append
    if "–∏–Ω—Ñ–ª—è—Ü" in t or "cpi" in t: a("Inflation"); a("Consumer price index")
    if "—Å—Ç–∞–≤–∫" in t or "—Ñ—Ä—Å" in t or "federal reserve" in t: a("Federal Reserve")
    if "–µ—Ü–±" in t or "european central bank" in t: a("European Central Bank")
    if "–±–∞–Ω–∫ —è–ø–æ–Ω–∏" in t or "boj" in t: a("Bank of Japan")
    if "–±–∞–Ω–∫ –∞–Ω–≥–ª–∏" in t: a("Bank of England")
    if "–Ω–µ—Ñ—Ç—å" in t or "brent" in t: a("Brent crude"); a("Petroleum")
    if "wti" in t: a("West Texas Intermediate")
    if "–≥–∞–∑" in t: a("Natural gas")
    if "–∑–æ–ª–æ—Ç–æ" in t or "xau" in t: a("Gold")
    if "–±–∏—Ç–∫–æ–∏–Ω" in t or "bitcoin" in t or "btc" in t: a("Bitcoin")
    if "ethereum" in t or "—ç—Ñ–∏—Ä–∏—É–º" in t or "eth" in t: a("Ethereum")
    if "stablecoin" in t or "usdt" in t: a("Tether (cryptocurrency)")
    if "–¥–æ–ª–ª–∞—Ä" in t or "usd" in t: a("United States dollar")
    if "–µ–≤—Ä–æ" in t or "eur" in t: a("Euro")
    if "—Ä—É–±–ª" in t or "rub" in t: a("Russian ruble")
    if "—é–∞–Ω—å" in t or "cny" in t: a("Renminbi")
    if "—Å–∞–Ω–∫—Ü" in t or "—ç–º–±–∞—Ä–≥–æ" in t: a("Sanctions (international relations)")
    return list(dict.fromkeys(titles))

def http_json(url, params=None, timeout=12):
    r = requests.get(url, params=params, headers=UA, timeout=timeout)
    if r.status_code != 200: return None
    return r.json()

def wiki_summary_image(title, lang="ru"):
    try:
        api = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{urllib.parse.quote(title)}"
        data = http_json(api, {})
        if not data: return None
        if "originalimage" in data and data["originalimage"].get("source"):
            return data["originalimage"]["source"]
        if "thumbnail" in data and data["thumbnail"].get("source"):
            return data["thumbnail"]["source"]
    except Exception:
        pass
    return None

def wiki_search_image(query, lang="ru"):
    url = wiki_summary_image(query, lang)
    if url: return url
    try:
        res = http_json(f"https://{lang}.wikipedia.org/w/rest.php/v1/search/title",
                        params={"q": query, "limit": 1})
        if res and res.get("pages"):
            key = res["pages"][0].get("key")
            if key:
                url = wiki_summary_image(key, lang)
                if url: return url
    except Exception:
        pass
    return None

def download_image(url, timeout=20):
    r = requests.get(url, headers=UA_IMG, timeout=timeout, allow_redirects=True)
    if r.status_code != 200: return None
    try:
        return Image.open(io.BytesIO(r.content)).convert("RGB")
    except Exception:
        return None

def resize_cover(img, W, H):
    w, h = img.size
    if w == 0 or h == 0: return img.resize((W,H))
    scale = max(W / w, H / h)
    new_w, new_h = int(w*scale), int(h*scale)
    img = img.resize((new_w, new_h), Image.LANCZOS)
    left = (new_w - W) // 2
    top  = (new_h - H) // 2
    return img.crop((left, top, left+W, top+H))

def unsplash_by_context(keywords, w=1080, h=540, retries=3):
    q = ",".join(keywords[:4]) if isinstance(keywords, list) else str(keywords)
    for i in range(retries):
        try:
            seed = random.randint(0, 10_000_000)
            url = f"https://source.unsplash.com/{w}x{h}/?{urllib.parse.quote(q)}&sig={seed}"
            r = requests.get(url, headers=UA_IMG, timeout=25, allow_redirects=True)
            if r.status_code == 200:
                return Image.open(io.BytesIO(r.content)).convert("RGB"), "source.unsplash.com"
            time.sleep(0.6*(i+1))
        except Exception:
            time.sleep(0.6*(i+1))
    return None, None

def picsum_fallback(w=1080, h=540):
    try:
        seed = random.randint(1, 10_000_000)
        url = f"https://picsum.photos/{w}/{h}?random={seed}"
        r = requests.get(url, headers=UA_IMG, timeout=20, allow_redirects=True)
        if r.status_code == 200:
            return Image.open(io.BytesIO(r.content)).convert("RGB"), "picsum.photos"
    except Exception:
        pass
    return None, None

def build_image_candidates(title, body, entities):
    text_l = f"{title} {body}".lower()
    cands = []
    for e in entities:
        cands.append(e)
    cands += topic_wiki_titles(text_l)
    for k,v in COUNTRY_PROPER.items():
        if k in text_l: cands.append(v)
    extra = []
    if "—Ä—ã–Ω–æ–∫" in text_l or "–∏–Ω–¥–µ–∫—Å" in text_l: extra += ["Stock market index","Stock exchange"]
    if "–æ–±–ª–∏–≥–∞—Ü" in text_l or "–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç" in text_l: extra += ["Government bond","Bond (finance)","Yield (finance)"]
    if "—Å–∞–Ω–∫—Ü" in text_l: extra += ["Sanctions (international relations)"]
    if "–±–∞–Ω–∫" in text_l and "—Ü–µ–Ω—Ç" in text_l: extra += ["Central bank"]
    cands += extra
    # —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è
    seen=set(); uniq=[]
    for x in cands:
        if x and x not in seen:
            seen.add(x); uniq.append(x)
    return uniq[:12]

def select_context_image(title, article_text_or_summary):
    entities = extract_entities(title, article_text_or_summary)
    candidates = build_image_candidates(title, article_text_or_summary, entities)

    # Wikipedia/Commons RU‚ÜíEN
    for cand in candidates:
        for lang in ("ru", "en"):
            url = wiki_search_image(cand, lang=lang)
            if url:
                img = download_image(url)
                if img:
                    return resize_cover(img, 1080, 540), "commons.wikimedia.org"

    # Unsplash
    kw = []
    for e in entities[:3]:
        kw.append(e)
    for t in topic_wiki_titles(f"{title} {article_text_or_summary}".lower())[:3]:
        kw.append(t)
    img, src = unsplash_by_context(kw or ["finance","markets","economy"])
    if img:
        return img, src

    # Picsum ‚Üí –ì—Ä–∞–¥–∏–µ–Ω—Ç
    img, src = picsum_fallback()
    if img: return img, src

    img = Image.new("RGB",(1080,540))
    d = ImageDraw.Draw(img)
    top=(24,26,28); bottom=(10,12,14)
    for y in range(540):
        a=y/539
        r=int(top[0]*(1-a)+bottom[0]*a)
        g=int(top[1]*(1-a)+bottom[1]*a)
        b=int(top[2]*(1-a)+bottom[2]*a)
        d.line([(0,y),(1080,y)], fill=(r,g,b))
    return img, "gradient"

# ========= –†–ï–ù–î–ï–† –ö–ê–†–¢–û–ß–ö–ò =========
def wrap_text_by_width(draw, text, font, max_width, max_lines=5):
    words = text.split()
    lines, current = [], ""
    for w in words:
        test = (current + " " + w).strip()
        if draw.textlength(test, font=font) <= max_width:
            current = test
        else:
            if current:
                lines.append(current)
                if len(lines) >= max_lines: return lines
            current = w
    if current and len(lines) < max_lines: lines.append(current)
    return lines

def fit_title_in_box(draw, text, font_path, box_w, box_h, start_size=64, min_size=28, line_gap=8, max_lines=5):
    for size in range(start_size, min_size-1, -2):
        font = ImageFont.truetype(font_path, size)
        lines = wrap_text_by_width(draw, text, font, box_w, max_lines=max_lines)
        h_line = font.getbbox("Ag")[3]
        total_h = len(lines)*h_line + (len(lines)-1)*line_gap
        if lines and total_h <= box_h: return font, lines
    font = ImageFont.truetype(font_path, min_size)
    lines = wrap_text_by_width(draw, text, font, box_w, max_lines=max_lines)
    return font, lines

def draw_title_card(title_text, src_domain, tzname, img_source_label, base_img):
    W, H = 1080, 540
    bg = base_img.copy()
    bg = ImageEnhance.Brightness(bg).enhance(0.9).filter(ImageFilter.GaussianBlur(radius=0.4))

    overlay = Image.new("RGBA", (W, H), (0,0,0,0))
    ImageDraw.Draw(overlay).rounded_rectangle([40, 110, W-40, H-90], radius=28, fill=(0,0,0,118))
    bg = Image.alpha_composite(bg.convert("RGBA"), overlay).convert("RGB")
    d = ImageDraw.Draw(bg)

    path_bold = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"
    path_reg  = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
    font_brand  = ImageFont.truetype(path_bold, 34)
    font_time   = ImageFont.truetype(path_reg, 26)
    font_small  = ImageFont.truetype(path_reg, 20)

    d.text((48, 26), CHANNEL_NAME, fill=(255,255,255), font=font_brand)
    try: tz = ZoneInfo(tzname)
    except Exception: tz = ZoneInfo("UTC")
    now_str = datetime.now(tz).strftime("%d.%m %H:%M")
    d.text((W - 48 - d.textlength(now_str, font=font_time), 26), now_str, fill=(255,255,255), font=font_time)

    box_x, box_y = 72, 150
    box_w, box_h = W - 2*box_x, H - box_y - 110
    font_title, lines = fit_title_in_box(d, (title_text or "").strip(), path_bold, box_w, box_h, start_size=64, min_size=30, max_lines=5)

    y = box_y
    for ln in lines:
        d.text((box_x, y), ln, font=font_title, fill=(255,255,255))
        y += font_title.getbbox("Ag")[3] + 8

    d.text((72, H - 58), f"source: {src_domain}", font=font_small, fill=(230,230,230))
    img_label = f"img: {img_source_label}"
    d.text((W - 72 - d.textlength(img_label, font=font_small), H - 58), img_label, font=font_small, fill=(230,230,230))

    bio = io.BytesIO(); bg.save(bio, format="PNG", optimize=True); bio.seek(0)
    return bio

# ========= –°–ö–ê–ß–ò–í–ê–ù–ò–ï –¢–ï–ö–°–¢–ê –°–¢–ê–¢–¨–ò =========
def fetch_article_text(url, max_chars=2600):
    try:
        r = requests.get(url, headers=UA, timeout=20)
        if r.status_code != 200: return ""
        soup = BeautifulSoup(r.text, "html.parser")
        ps = soup.find_all("p")
        chunks = []
        for p in ps:
            t = p.get_text(" ", strip=True)
            if not t or len(t) < 60: continue
            if any(x in t.lower() for x in ["javascript","cookie","–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å","—Ä–µ–∫–ª–∞–º–∞","cookies"]): continue
            chunks.append(t)
            if sum(len(c) for c in chunks) > max_chars: break
        text = " ".join(chunks)
        return re.sub(r"\s+", " ", text).strip()
    except Exception:
        return ""

# ========= –°–¢–ò–õ–¨ –¢–ï–ö–°–¢–ê (¬´–Ω–∞—É—á–Ω—ã–π¬ª) =========
RU_TONE_REWRITE = [
    (r"\b—Å–∫–∞–∑–∞–ª(–∞|–∏)?\b", "—Å–æ–æ–±—â–∏–ª\\1"),
    (r"\b–∑–∞—è–≤–∏–ª(–∞|–∏)?\b", "–æ—Ç–º–µ—Ç–∏–ª\\1"),
    (r"\b–ø–æ —Å–ª–æ–≤–∞–º\b", "–ø–æ –¥–∞–Ω–Ω—ã–º"),
    (r"\b–ø–æ –º–Ω–µ–Ω–∏—é\b", "—Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ü–µ–Ω–∫–∞–º"),
    (r"\b–ø—Ä–∏–º–µ—Ä–Ω–æ\b", "–ø–æ—Ä—è–¥–∫–∞"),
    (r"\b–æ—á–µ–Ω—å\b", "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ"),
    (r"\b—Å–∏–ª—å–Ω–æ\b", "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ"),
]
def ru_scientific_paraphrase(s):
    out = s
    for pat, repl in RU_TONE_REWRITE:
        out = re.sub(pat, repl, out, flags=re.IGNORECASE)
    out = re.sub(r"\s+%", "%", out)
    return re.sub(r"\s+", " ", out).strip()

def split_sentences(text):
    text = re.sub(r"\s+", " ", text).strip()
    if not text: return []
    return re.split(r"(?<=[.!?])\s+", text)

def paraphrase_sentence_ru_or_en(s):
    if detect_lang(s) == "en":
        s = translate_en_to_ru(s)
    return ru_scientific_paraphrase(s)

def one_context_emoji(context):
    t = (context or "").lower()
    if any(k in t for k in ["–±–∏—Ç–∫–æ–∏–Ω","crypto","btc","ethereum","–∫—Ä–∏–ø—Ç"]): return "ü™ô"
    if any(k in t for k in ["–∞–∫—Ü–∏","–∏–Ω–¥–µ–∫—Å","—Ä—ã–Ω–æ–∫","–±–∏—Ä–∂","nasdaq","nyse","s&p"]): return "üìà"
    if any(k in t for k in ["–¥–æ–ª–ª–∞—Ä","—Ä—É–±–ª","–≤–∞–ª—é—Ç","–∫—É—Ä—Å","–µ–≤—Ä–æ","—é–∞–Ω—å","usd","eur","cny"]): return "üíµ"
    if any(k in t for k in ["—Å—Ç–∞–≤–∫","—Ñ—Ä—Å","—Ü–±","—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫","–∏–Ω—Ñ–ª—è—Ü","cpi","ppi"]): return "üè¶"
    if any(k in t for k in ["–Ω–µ—Ñ—Ç—å","–±—Ä–µ–Ω—Ç","wti","opec","–≥–∞–∑","lng","—ç–Ω–µ—Ä–≥–∏"]): return "üõ¢Ô∏è"
    if any(k in t for k in ["–∑–æ–ª–æ—Ç–æ","xau","–º–µ—Ç–∞–ª–ª","—Å–µ—Ä–µ–±—Ä–æ"]): return "ü•á"
    if any(k in t for k in ["—Å–∞–Ω–∫—Ü","—ç–º–±–∞—Ä–≥–æ","–ø–æ—à–ª–∏–Ω","–≥–µ–æ–ø–æ–ª–∏—Ç","–ø–µ—Ä–µ–≥–æ–≤–æ—Ä","–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"]): return "üèõÔ∏è"
    return "üì∞"

def build_three_paragraphs_scientific(title, article_text, feed_summary):
    base = (article_text or "").strip() or (feed_summary or "").strip()
    if detect_lang(base) == "en": base = translate_en_to_ru(base)
    sents = [s for s in split_sentences(base) if s]

    p1_src = sents[:2] or sents[:1]
    p2_src = sents[2:5] or sents[:1]
    p3_src = sents[5:8] or sents[1:3] or sents[:1]

    p1 = " ".join(paraphrase_sentence_ru_or_en(s) for s in p1_src)
    p2 = " ".join(paraphrase_sentence_ru_or_en(s) for s in p2_src)
    p3 = " ".join(paraphrase_sentence_ru_or_en(s) for s in p3_src)

    emoji = one_context_emoji(f"{title} {base}")
    return f"{emoji} {clamp(p1, 320)}", clamp(p2, 360), clamp(p3, 360)

# ========= –¢–ï–ì–ò: —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ, –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂ =========
RU_STOP = set("—ç—Ç–æ —Ç–æ—Ç —ç—Ç–∞ –∫–æ—Ç–æ—Ä—ã–µ –∫–æ—Ç–æ—Ä—ã–π –∫–æ—Ç–æ—Ä–æ–π –∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞–∫–∂–µ —á—Ç–æ–±—ã –ø—Ä–∏ –ø—Ä–æ –¥–ª—è –Ω–∞ –∏–∑ –æ—Ç –ø–æ –∫–∞–∫ —É–∂–µ –µ—â–µ –∏–ª–∏ –ª–∏–±–æ —á–µ–º –µ—Å–ª–∏ –∫–æ–≥–¥–∞ –≥–¥–µ –∫—É–¥–∞ –≤–µ—Å—å –≤—Å–µ –≤—Å—è –µ–≥–æ –µ–µ –∏—Ö –Ω–∞—à –≤–∞—à –º–æ–π —Ç–≤–æ–π –æ–¥–∏–Ω –æ–¥–Ω–∞ –æ–¥–Ω–æ".split())
def lemma_noun(word):
    w = word.lower()
    if MORPH:
        p = MORPH.parse(w)[0]
        if 'NOUN' in p.tag:
            nf = p.normal_form
            if nf in COUNTRY_PROPER: return COUNTRY_PROPER[nf]  # —Å—Ç—Ä–∞–Ω—ã ‚Äî —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
            return nf
    return w

def extract_candidate_nouns(text, entities, limit=10):
    words = re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]{3,}", text)
    candidates = []
    for w in words:
        wl = w.lower()
        if wl in RU_STOP: continue
        candidates.append(wl)
    for e in entities:
        if re.fullmatch(r"[A-Z]{2,6}", e):
            candidates.append(e)
        else:
            candidates += e.split()
    lemmas = []
    for c in candidates:
        if re.fullmatch(r"[A-Z]{2,6}", c):
            lemmas.append(c)
        else:
            l = lemma_noun(c)
            if l and len(l) >= 3:
                lemmas.append(l)
    freq = {}
    for l in lemmas:
        freq[l] = freq.get(l, 0) + 1
    out = [k for k,_ in sorted(freq.items(), key=lambda x: -x[1])]
    out = [re.sub(r"[^A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]", "", x) for x in out]
    out = [x for x in out if x and x.lower() not in RU_STOP]
    return out[:limit]

def gen_tags_nominative(title, body, entities, max_tags=6):
    text_l = (title + " " + body).lower()
    thematic = []
    def tadd(x):
        if x not in thematic: thematic.append(x)

    if any(k in text_l for k in ["–±–∏—Ç–∫–æ–∏–Ω","bitcoin","btc","–∫—Ä–∏–ø—Ç","ethereum","eth","stablecoin"]): tadd("–∫—Ä–∏–ø—Ç–∞")
    if any(k in text_l for k in ["–¥–æ–ª–ª–∞—Ä","usd","–µ–≤—Ä–æ","eur","—Ä—É–±–ª","rub","—é–∞–Ω—å","cny","–∫—É—Ä—Å","—Ñ–æ—Ä–µ–∫—Å"]): tadd("–≤–∞–ª—é—Ç–∞")
    if any(k in text_l for k in ["–∞–∫—Ü–∏","—Ä—ã–Ω–æ–∫","–±–∏—Ä–∂","–∏–Ω–¥–µ–∫—Å","nasdaq","nyse","s&p","sp500","dow"]): tadd("—Ä—ã–Ω–∫–∏")
    if any(k in text_l for k in ["—Å—Ç–∞–≤–∫","—Ñ—Ä—Å","—Ü–±","–∏–Ω—Ñ–ª—è—Ü","cpi","ppi","qe","qt"]): tadd("—Å—Ç–∞–≤–∫–∏")
    if any(k in text_l for k in ["–Ω–µ—Ñ—Ç—å","–±—Ä–µ–Ω—Ç","wti","opec","–≥–∞–∑","—ç–Ω–µ—Ä–≥–∏","lng"]): tadd("—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
    if any(k in text_l for k in ["—Å–∞–Ω–∫—Ü","—ç–º–±–∞—Ä–≥–æ","–ø–æ—à–ª–∏–Ω","–≥–µ–æ–ø–æ–ª–∏—Ç","–ø–µ—Ä–µ–≥–æ–≤–æ—Ä","–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"]): tadd("–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞")

    nouns = extract_candidate_nouns(title + " " + body, entities, limit=12)

    result = []
    def add(s):
        if s and s not in result and len(result) < max_tags:
            result.append(s)
    for t in thematic: add(t)
    for n in nouns: add(COUNTRY_PROPER.get(n.lower(), n))

    tags=[]
    for t in result[:max_tags]:
        if re.fullmatch(r"[A-Z]{2,6}", t):
            tags.append("#"+t)
        else:
            if t in COUNTRY_PROPER.values():
                tags.append("#"+t)        # —Å—Ç—Ä–∞–Ω—ã ‚Äî —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
            else:
                tags.append("#"+t.lower())  # –æ–±—â–∏–µ —Ç–µ–º—ã ‚Äî —Å—Ç—Ä–æ—á–Ω—ã–º–∏
    return " ".join(tags[:max_tags])

# ========= –ö–ê–ü–®–ï–ù =========
def build_caption(title, para1, para2, para3, link, tags_str):
    title = clamp(title, 200)
    dom = root_domain(link) if link else None
    body = f"{para1}\n\n{para2}\n\n{para3}"

    parts = [title, "", body]
    if dom: parts += ["", f"–ò—Å—Ç–æ—á–Ω–∏–∫: [{dom}]({link})"]
    else:   parts += ["", "–ò—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"]
    parts += ["", f"[{CHANNEL_NAME}]({CHANNEL_LINK})"]
    if tags_str: parts += ["", tags_str]

    cap = "\n".join(parts)
    if len(cap) > 1024:
        over = len(cap) - 1024 + 3
        p3 = clamp(para3[:-min(over, len(para3))], 300)
        parts = [title, "", f"{para1}\n\n{para2}\n\n{p3}"]
        parts += ["", f"–ò—Å—Ç–æ—á–Ω–∏–∫: [{dom}]({link})" if dom else "–ò—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                  "", f"[{CHANNEL_NAME}]({CHANNEL_LINK})", "", tags_str]
        cap = "\n".join(parts)
        if len(cap) > 1024:
            over = len(cap) - 1024 + 3
            p2 = clamp(para2[:-min(over, len(para2))], 300)
            parts = [title, "", f"{para1}\n\n{p2}\n\n{p3}"]
            parts += ["", f"–ò—Å—Ç–æ—á–Ω–∏–∫: [{dom}]({link})" if dom else "–ò—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                      "", f"[{CHANNEL_NAME}]({CHANNEL_LINK})", "", tags_str]
            cap = "\n".join(parts)
            if len(cap) > 1024:
                over = len(cap) - 1024 + 3
                p1 = clamp(para1[:-min(over, len(para1))], 280)
                parts = [title, "", f"{p1}\n\n{p2}\n\n{p3}"]
                parts += ["", f"–ò—Å—Ç–æ—á–Ω–∏–∫: [{dom}]({link})" if dom else "–ò—Å—Ç–æ—á–Ω–∏–∫: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                          "", f"[{CHANNEL_NAME}]({CHANNEL_LINK})", "", tags_str]
                cap = "\n".join(parts)
    return cap

# ========= –¢–ï–õ–ï–ì–†–ê–ú =========
def send_photo(photo_bytes, caption):
    if not BOT_TOKEN:
        raise RuntimeError("–ù–µ—Ç BOT_TOKEN. –î–æ–±–∞–≤—å —Å–µ–∫—Ä–µ—Ç BOT_TOKEN –≤ GitHub ‚Üí Settings ‚Üí Secrets ‚Üí Actions")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto"
    files = {"photo": ("cover.png", photo_bytes, "image/png")}
    data = {"chat_id": CHANNEL_ID, "caption": caption, "parse_mode": "Markdown"}
    r = requests.post(url, files=files, data=data, timeout=30)
    print("Telegram status:", r.status_code, r.text[:200])
    r.raise_for_status()
    return r.json()

# ========= –°–ë–û–† –§–ò–î–û–í =========
def collect_entries():
    items = []
    for feed_url in RSS_FEEDS:
        try:
            fp = feedparser.parse(feed_url)
        except Exception:
            continue
        for e in fp.entries or []:
            link = getattr(e, "link", "") or ""
            title = (getattr(e, "title", "") or "").strip()
            summary = clean_html(getattr(e, "summary", getattr(e, "description", "")))
            ts = getattr(e, "published", getattr(e, "updated", "")) or ""
            try:
                dt = dtparse.parse(ts)
                if not dt.tzinfo:
                    dt = dt.replace(tzinfo=timezone.utc)
            except Exception:
                dt = datetime(1970,1,1, tzinfo=timezone.utc)
            uid = hashlib.sha256((link + "|" + title + "|" + ts).encode("utf-8")).hexdigest()
            items.append({"feed": feed_url, "link": link, "title": title or "(no title)",
                          "summary": summary, "ts": ts, "dt": dt, "uid": uid})
    return items

# ========= –û–ë–†–ê–ë–û–¢–ö–ê –û–î–ù–û–ô –ù–û–í–û–°–¢–ò =========
def process_item(link, title, feed_summary):
    article_text = fetch_article_text(link, max_chars=2600)
    p1, p2, p3 = build_three_paragraphs_scientific(title, article_text, feed_summary)

    # –¢–µ–≥–∏ (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ, –ò.–ø.)
    entities_for_tags = extract_entities(title, f"{p1} {p2} {p3}")
    tags_str = gen_tags_nominative(title, f"{p1} {p2} {p3}", entities_for_tags, max_tags=6) or "#–Ω–æ–≤–æ—Å—Ç–∏"

    # –§–æ—Ç–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    img_obj, img_src_label = select_context_image(title, article_text or feed_summary or "")
    card = draw_title_card(title, domain(link or ""), TIMEZONE, img_src_label, img_obj)

    caption = build_caption(title, p1, p2, p3, link or "", tags_str)
    resp = send_photo(card, caption)
    print(f"Posted: {title[:80]} | img={img_src_label} | tags={tags_str}")

# ========= MAIN =========
def trim_posted(posted_set, keep_last=600):
    if len(posted_set) <= keep_last: return posted_set
    return set(list(posted_set)[-keep_last:])

def main():
    state = load_state()
    posted = set(state.get("posted_uids", []))

    items = collect_entries()
    if not items:
        print("No entries found."); return

    now = datetime.now(timezone.utc)
    lookback_dt = now - timedelta(minutes=LOOKBACK_MINUTES)
    fresh = [it for it in items if it["dt"] >= lookback_dt and it["uid"] not in posted]

    fresh.sort(key=lambda x: x["dt"], reverse=True)
    to_post = fresh[:MAX_POSTS_PER_RUN]
    if not to_post:
        print("Nothing new to post within lookback window."); return

    for it in to_post:
        try:
            process_item(it["link"], it["title"], it["summary"])
            posted.add(it["uid"])
            time.sleep(1.0)
        except Exception as e:
            print("Error sending:", e)

    state["posted_uids"] = list(trim_posted(posted))
    save_state(state)

if __name__ == "__main__":
    main()
