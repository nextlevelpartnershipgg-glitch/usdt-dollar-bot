# bot/poster.py
import os, io, json, time, pathlib, hashlib, urllib.parse, random, re
from datetime import datetime, timezone, timedelta
from dateutil import parser as dtparse
import feedparser, requests
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
from zoneinfo import ZoneInfo

# ========= –ù–ê–°–¢–†–û–ô–ö–ò =========
BOT_TOKEN  = os.environ.get("BOT_TOKEN")
CHANNEL_ID = os.environ.get("CHANNEL_ID", "@usdtdollarm")
TIMEZONE   = os.environ.get("TIMEZONE", "Europe/Moscow")

CHANNEL_NAME   = os.environ.get("CHANNEL_NAME", "USDT=Dollar")
CHANNEL_HANDLE = os.environ.get("CHANNEL_HANDLE", "@usdtdollarm")
CHANNEL_LINK   = os.environ.get("CHANNEL_LINK", f"https://t.me/{CHANNEL_HANDLE.lstrip('@')}")

MAX_POSTS_PER_RUN  = int(os.environ.get("MAX_POSTS_PER_RUN", "1"))
LOOKBACK_MINUTES   = int(os.environ.get("LOOKBACK_MINUTES", "30"))
FRESH_WINDOW_MIN   = int(os.environ.get("FRESH_WINDOW_MIN", "25"))
MIN_EVENT_YEAR     = int(os.environ.get("MIN_EVENT_YEAR", "2023"))

FALLBACK_ON_NO_FRESH = os.environ.get("FALLBACK_ON_NO_FRESH", "1") == "1"
FALLBACK_WINDOW_MIN  = int(os.environ.get("FALLBACK_WINDOW_MIN", "360"))  # 6 —á–∞—Å–æ–≤
ALWAYS_POST          = os.environ.get("ALWAYS_POST", "1") == "1"

DATA_DIR = pathlib.Path("data"); DATA_DIR.mkdir(parents=True, exist_ok=True)
STATE_FILE   = DATA_DIR / "state.json"
HISTORY_FILE = DATA_DIR / "history.json"

UA = {"User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}

# ========= –ò–°–¢–û–ß–ù–ò–ö–ò (–†–§ + –º–∏—Ä; –±–µ–∑ –†–ò–ê) =========
# ===== –¢–µ–∫—Å—Ç: –∫–∞–ø—Å –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã + –±–∞–ª–∞–Ω—Å —Å–∫–æ–±–æ–∫/–∫–∞–≤—ã—á–µ–∫ =====
def _smart_capitalize(s: str) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if not s:
        return s
    m = re.search(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]", s)
    if not m:
        return s
    i = m.start()
    return s[:i] + s[i].upper() + s[i+1:]

def _remove_unmatched(s: str, open_ch: str, close_ch: str) -> str:
    bal = 0
    out = []
    for ch in s:
        if ch == open_ch:
            bal += 1
            out.append(ch)
        elif ch == close_ch:
            if bal == 0:
                # –ª–∏—à–Ω—è—è –∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º
                continue
            bal -= 1
            out.append(ch)
        else:
            out.append(ch)
    # –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ ‚Äî –¥–æ–±–∞–≤–∏–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ –≤ –∫–æ–Ω–µ—Ü
    if bal > 0:
        out.append(close_ch * bal)
    return "".join(out)

def _balance_brackets_and_quotes(s: str) -> str:
    s = _remove_unmatched(s, "(", ")")
    s = _remove_unmatched(s, "[", "]")
    # —Ä—É—Å—Å–∫–∏–µ ¬´—ë–ª–æ—á–∫–∏¬ª
    opens = s.count("¬´"); closes = s.count("¬ª")
    if closes > opens:
        # –≤—ã–∫–∏–Ω—É—Ç—å –ª–∏—à–Ω–∏–µ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ ¬´¬ª
        need = opens
        buf = []
        seen = 0
        for ch in s:
            if ch == "¬ª":
                if seen >= need:
                    continue
                seen += 1
            buf.append(ch)
        s = "".join(buf)
    elif opens > closes:
        s += "¬ª" * (opens - closes)
    return s

def tidy_paragraph(p: str) -> str:
    p = (p or "").strip()
    if not p:
        return p
    p = _balance_brackets_and_quotes(p)
    p = _smart_capitalize(p)
    return p

# ========= –ò–°–¢–û–ß–ù–ò–ö–ò (—Ç–æ–ª—å–∫–æ –†–§, –≤—Å–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º) =========
RSS_FEEDS_RU = [
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
    "https://rssexport.rbc.ru/rbcnews/economics/30/full.rss",
    "https://rssexport.rbc.ru/rbcnews/finance/30/full.rss",
    "https://www.kommersant.ru/RSS/news.xml",
    "https://www.kommersant.ru/RSS/economics.xml",
    "https://www.kommersant.ru/RSS/finance.xml",
    "https://lenta.ru/rss/news",
    "https://lenta.ru/rss/economics",
    "https://lenta.ru/rss/russia",
    "https://lenta.ru/rss/world",
    "https://tass.ru/rss/v2.xml",
    "https://tass.ru/economy/rss",
    "https://tass.ru/politika/rss",
    "https://www.vedomosti.ru/rss/news",
    "https://www.interfax.ru/rss.asp",
    "https://www.gazeta.ru/export/rss/first.xml",
    "https://www.gazeta.ru/export/rss/business.xml",
    "https://iz.ru/xml/rss/all.xml",
    "https://www.finmarket.ru/rss/news.asp",
    "https://www.banki.ru/xml/news.rss",
    "https://1prime.ru/export/rss2/index.xml",
    "https://rg.ru/tema/ekonomika/rss.xml",
    "https://www.forbes.ru/newrss.xml",
    "https://www.mskagency.ru/rss/all",
    "https://www.ng.ru/rss/",
    "https://www.mk.ru/rss/finance/index.xml",
    "https://www.kommersant.ru/RSS/regions.xml",
    "https://www.kommersant.ru/RSS/tech.xml",
    "https://www.fontanka.ru/fontanka.rss",
    "https://minfin.gov.ru/ru/press-center/?rss=Y",
    "https://cbr.ru/StaticHtml/Rss/Press",
    "https://www.moex.com/Export/MRSS/News",
]

# –ú–∏—Ä–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —É–±–∏—Ä–∞–µ–º:
RSS_FEEDS_WORLD = []

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ RU:
RSS_FEEDS = RSS_FEEDS_RU

# ========= Pymorphy (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) =========
try:
    import pymorphy2
    MORPH = pymorphy2.MorphAnalyzer()
except Exception:
    MORPH = None

# ========= –£—Ç–∏–ª–∏—Ç—ã =========
def load_state():
    if STATE_FILE.exists():
        return json.loads(STATE_FILE.read_text(encoding="utf-8"))
    return {}

def save_state(state):
    STATE_FILE.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def load_history():
    if HISTORY_FILE.exists():
        try:
            return json.loads(HISTORY_FILE.read_text(encoding="utf-8"))
        except Exception:
            return []
    return []

def append_history(entry):
    hist = load_history()
    hist.append(entry)
    HISTORY_FILE.write_text(json.dumps(hist, ensure_ascii=False, indent=2), encoding="utf-8")

def domain(url):
    return urllib.parse.urlparse(url).netloc.replace("www.", "") or "source"

def root_domain(url):
    try:
        dom = urllib.parse.urlparse(url).netloc.replace("www.","")
        parts = dom.split(".")
        if len(parts) > 2: dom = ".".join(parts[-2:])
        return dom
    except Exception:
        return "–∏—Å—Ç–æ—á–Ω–∏–∫"

def clean_html(html):
    if not html: return ""
    soup = BeautifulSoup(html, "html.parser")
    return " ".join(soup.get_text(separator=" ").split())

# ========= –ü–µ—Ä–µ–≤–æ–¥ EN‚ÜíRU =========
def detect_lang(text: str) -> str:
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", text): return "ru"
    en_hits = len(re.findall(r"\b(the|and|of|to|in|for|on|with|from|by|as|at|is|are|this|that|it|was|be)\b", text.lower()))
    ru_hits = len(re.findall(r"\b(–∏|–≤|–Ω–∞|–ø–æ|–¥–ª—è|–∏–∑|–æ—Ç|–∫–∞–∫|—ç—Ç–æ|—á—Ç–æ|–±—ã|–Ω–µ|–∫|—Å|–æ|–æ–±)\b", text.lower()))
    return "en" if en_hits > ru_hits else "ru"

LT_ENDPOINTS = [
    "https://libretranslate.de/translate",
    "https://translate.argosopentech.com/translate",
]
LOCAL_EN_RU = {
    "china":"–ö–∏—Ç–∞–π","beijing":"–ü–µ–∫–∏–Ω","central bank":"—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫","central banks":"—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ –±–∞–Ω–∫–∏",
    "dollar":"–¥–æ–ª–ª–∞—Ä","us dollar":"–¥–æ–ª–ª–∞—Ä –°–®–ê","reserve":"—Ä–µ–∑–µ—Ä–≤","reserves":"—Ä–µ–∑–µ—Ä–≤—ã","safe haven":"—Ç–∏—Ö–∞—è –≥–∞–≤–∞–Ω—å",
    "gold":"–∑–æ–ª–æ—Ç–æ","gold futures":"—Ñ—å—é—á–µ—Ä—Å—ã –Ω–∞ –∑–æ–ª–æ—Ç–æ","comex":"Comex","ounce":"—É–Ω—Ü–∏—è","billion":"–º–ª—Ä–¥",
    "percent":"%","percentage":"%","share":"–¥–æ–ª—è","holdings":"–∑–∞–ø–∞—Å—ã","treasuries":"–∫–∞–∑–Ω–∞—á–µ–π—Å–∫–∏–µ –æ–±–ª–∏–≥–∞—Ü–∏–∏",
    "alternative":"–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞","geopolitical":"–≥–µ–æ–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π","risk":"—Ä–∏—Å–∫","risks":"—Ä–∏—Å–∫–∏",
    "inflation":"–∏–Ω—Ñ–ª—è—Ü–∏—è","stability":"—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å","assets":"–∞–∫—Ç–∏–≤—ã","backed":"–æ–±–µ—Å–ø–µ—á–µ–Ω–Ω—ã–π",
    "increase":"—Ä–æ—Å—Ç","rose":"–≤—ã—Ä–æ—Å","rise":"—Ä–æ—Å—Ç","jump":"—Å–∫–∞—á–æ–∫","month":"–º–µ—Å—è—Ü","monthly":"–µ–∂–µ–º–µ—Å—è—á–Ω—ã–π",
}
def translate_hard_ru(text: str, timeout=14) -> str:
    text = (text or "").strip()
    if not text: return text
    for ep in LT_ENDPOINTS:
        try:
            r = requests.post(ep, data={"q": text, "source":"en", "target":"ru", "format":"text"},
                              headers={"Accept":"application/json"}, timeout=timeout)
            if r.status_code == 200:
                out = (r.json() or {}).get("translatedText", "")
                if out and detect_lang(out) == "ru": return out.strip()
        except Exception:
            continue
    s = text
    for k in sorted(LOCAL_EN_RU.keys(), key=lambda x: -len(x)):
        s = re.sub(rf"\b{re.escape(k)}\b", LOCAL_EN_RU[k], s, flags=re.IGNORECASE)
    if detect_lang(s) == "en":
        s = "–ü–µ—Ä–µ–≤–æ–¥ (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π): " + s
    return s
def ensure_russian(text: str) -> str:
    return translate_hard_ru(text) if detect_lang(text) == "en" else text


# ===== –ö–æ—Ä–æ—Ç–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ (1‚Äì2 —Ñ—Ä–∞–∑—ã, –±–µ–∑ –¥–æ–º—ã—Å–ª–æ–≤) =====
def _sentiment_hint(text: str) -> str:
    t = (text or "").lower()
    neg = any(k in t for k in [
        "–ø–∞–¥–µ–Ω", "—Å–Ω–∏–∂–µ–Ω", "—Å–æ–∫—Ä–∞—â", "—à—Ç—Ä–∞—Ñ", "—Å–∞–Ω–∫—Ü", "—É–±—ã—Ç", "–¥–µ—Ñ–∏—Ü",
        "–æ—Ç–∑—ã–≤", "–∫—Ä–∏–∑–∏—Å", "–Ω–µ—É—Å—Ç–æ–π—á–∏–≤", "–∑–∞–º–µ–¥–ª–µ–Ω"
    ])
    pos = any(k in t for k in [
        "—Ä–æ—Å—Ç", "—É–≤–µ–ª–∏—á", "—Ä–∞—Å—à–∏—Ä", "—Ä–µ–∫–æ—Ä–¥", "–æ–¥–æ–±—Ä–µ–Ω", "–ø—Ä–∏–±—ã–ª",
        "–ø–ª–∞–Ω–∏—Ä—É–µ—Ç", "—É—Å–∫–æ—Ä–µ–Ω", "—É–ª—É—á—à–µ–Ω", "–ø–æ–≤—ã—à–µ–Ω"
    ])
    if pos and not neg:
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ-–ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è"
    if neg and not pos:
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ-–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è"
    return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è"

def generate_brief_analysis(title_ru: str, p1: str, p2: str, p3: str) -> str:
    """
    –î–µ–ª–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥ –ø–æ —Ñ–∞–∫—Ç–∞–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –Ω–æ–≤—ã—Ö —Å–≤–µ–¥–µ–Ω–∏–π).
    –¢–æ–Ω: –¥–µ–ª–æ–≤–æ–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π. 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    """
    body = " ".join([p1 or "", p2 or "", p3 or ""])
    mood = _sentiment_hint(body)

    # –±–∞–∑–æ–≤–∞—è —Ç–µ–º–∞ –ø–æ –∫–ª—é—á–∞–º
    topic = "—Ä—ã–Ω–æ–∫"
    tl = (title_ru + " " + body).lower()
    if any(w in tl for w in ["—Å—Ç–∞–≤–∫", "—Ü–±", "—Ñ—Ä—Å", "–∏–Ω—Ñ–ª—è—Ü"]):
        topic = "–¥–µ–Ω–µ–∂–Ω–æ-–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞"
    elif any(w in tl for w in ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "opec", "–±—Ä–µ–Ω—Ç", "—ç–Ω–µ—Ä–≥–∏", "lng"]):
        topic = "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"
    elif any(w in tl for w in ["–∞–∫—Ü–∏", "–±–∏—Ä–∂", "–∏–Ω–¥–µ–∫—Å", "nasdaq", "moex", "s&p", "–æ–±–ª–∏–≥–∞—Ü"]):
        topic = "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä—ã–Ω–∫–∏"
    elif any(w in tl for w in ["–∫—Ä–∏–ø—Ç", "–±–∏—Ç–∫–æ–∏–Ω", "bitcoin", "eth", "—Å—Ç–µ–π–±–ª"]):
        topic = "–∫—Ä–∏–ø—Ç–æ—Ä—ã–Ω–æ–∫"
    elif any(w in tl for w in ["–±—é–¥–∂–µ—Ç", "–Ω–∞–ª–æ–≥", "–º–∏–Ω—Ñ–∏–Ω", "—Ä–∞—Å—Ö–æ–¥", "–¥–æ—Ö–æ–¥"]):
        topic = "–≥–æ—Å—Ñ–∏–Ω–∞–Ω—Å—ã"
    elif any(w in tl for w in ["–≤–≤–ø", "–±–µ–∑—Ä–∞–±–æ—Ç", "–¥–µ–ª–æ–≤", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤", "—ç–∫—Å–ø–æ—Ä—Ç", "–∏–º–ø–æ—Ä—Ç"]):
        topic = "–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞"

    a1 = f"–ò—Ç–æ–≥ ({topic}, {mood}): —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –æ–ø–∏—Å–∞–Ω–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ç–µ–∫—É—â—É—é –¥–∏–Ω–∞–º–∏–∫—É –±–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤."
    a2 = "–ö—Ä–∏—Ç–∏—á–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–µ–ª–∏–∑–∞–º–∏ –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞."
    return a1 + " " + a2


# ========= –°—É—â–Ω–æ—Å—Ç–∏/—Ç–µ–≥–∏ =========
COMPANY_HINTS = ["Apple","Microsoft","Tesla","Meta","Google","Alphabet","Amazon","Nvidia","Samsung","Intel","Huawei",
                 "–ì–∞–∑–ø—Ä–æ–º","–°–±–µ—Ä–±–∞–Ω–∫","–Ø–Ω–¥–µ–∫—Å","–†–æ—Å–Ω–µ—Ñ—Ç—å","–õ—É–∫–æ–π–ª","–ù–æ—Ä–Ω–∏–∫–µ–ª—å","–¢–∞—Ç–Ω–µ—Ñ—Ç—å","–ù–æ–≤–∞—Ç—ç–∫","–í–¢–ë","–°—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑"]
TICKER_PAT = re.compile(r"\b[A-Z]{2,6}\b")

RU_STOP=set("—ç—Ç–æ —Ç–æ—Ç —ç—Ç–∞ –∫–æ—Ç–æ—Ä—ã–µ –∫–æ—Ç–æ—Ä—ã–π –∫–æ—Ç–æ—Ä–æ–π –∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞–∫–∂–µ —á—Ç–æ–±—ã –ø—Ä–∏ –ø—Ä–æ –¥–ª—è –Ω–∞ –∏–∑ –æ—Ç –ø–æ –∫–∞–∫ —É–∂–µ –µ—â–µ –∏–ª–∏ –ª–∏–±–æ —á–µ–º –µ—Å–ª–∏ –∫–æ–≥–¥–∞ –≥–¥–µ –∫—É–¥–∞ –≤–µ—Å—å –≤—Å–µ –≤—Å—è –µ–≥–æ –µ–µ –∏—Ö –Ω–∞—à –≤–∞—à –º–æ–π —Ç–≤–æ–π –æ–¥–∏–Ω –æ–¥–Ω–∞ –æ–¥–Ω–æ".split())
COUNTRY_PROPER={"—Ä–æ—Å—Å–∏—è":"–†–æ—Å—Å–∏—è","—Å—à–∞":"–°–®–ê","–∫–∏—Ç–∞–π":"–ö–∏—Ç–∞–π","—è–ø–æ–Ω–∏—è":"–Ø–ø–æ–Ω–∏—è","–≥–µ—Ä–º–∞–Ω–∏—è":"–ì–µ—Ä–º–∞–Ω–∏—è","—Ñ—Ä–∞–Ω—Ü–∏—è":"–§—Ä–∞–Ω—Ü–∏—è",
                "–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è":"–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è","–∏–Ω–¥–∏—è":"–ò–Ω–¥–∏—è","–µ–≤—Ä–æ–ø–∞":"–ï–≤—Ä–æ–ø–∞","—É–∫—Ä–∞–∏–Ω–∞":"–£–∫—Ä–∞–∏–Ω–∞","—Ç—É—Ä—Ü–∏—è":"–¢—É—Ä—Ü–∏—è"}

def extract_entities(title, summary):
    text = f"{title} {summary}".strip()
    names = re.findall(r"(?:[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+(?:\s+[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+){0,2})", text)
    tickers = [m for m in TICKER_PAT.findall(text) if m not in ("NEWS","HTTP","HTTPS","HTML")]
    companies = [c for c in COMPANY_HINTS if c.lower() in text.lower()]
    stop = {"The","This"}
    names = [x for x in names if x not in stop and len(x) > 2]
    out = []; out += names[:5]; out += companies[:5]; out += tickers[:5]
    seen=set(); uniq=[]
    for x in out:
        if x not in seen: seen.add(x); uniq.append(x)
    return uniq or ["—Ä—ã–Ω–∫–∏","—ç–∫–æ–Ω–æ–º–∏–∫–∞"]

def lemma_noun(word):
    w=word.lower()
    try:
        if MORPH:
            p=MORPH.parse(w)[0]
            if "NOUN" in p.tag:
                nf=p.normal_form
                return COUNTRY_PROPER.get(nf, nf)
    except Exception:
        pass
    return w

def extract_candidate_nouns(text, entities, limit=12):
    words=re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]{3,}", text)
    candidates=[w.lower() for w in words if w.lower() not in RU_STOP]
    for e in entities:
        if re.fullmatch(r"[A-Z]{2,6}", e): candidates.append(e)
        else: candidates += e.split()
    lemmas=[]
    for c in candidates:
        if re.fullmatch(r"[A-Z]{2,6}", c): lemmas.append(c)
        else:
            l=lemma_noun(c)
            if l and len(l)>=3: lemmas.append(l)
    freq={}
    for l in lemmas: freq[l]=freq.get(l,0)+1
    out=[k for k,_ in sorted(freq.items(), key=lambda x: -x[1])]
    out=[re.sub(r"[^A-Za-z–ê-–Ø–∞-—è–Å—ë0-9]","",x) for x in out]
    out=[x for x in out if x and x.lower() not in RU_STOP]
    return out[:limit]

def gen_hidden_tags(title, body, entities, min_tags=3, max_tags=5):
    text_l = (title + " " + body).lower()
    thematic=[]
    def tadd(x):
        if x not in thematic: thematic.append(x)
    if any(k in text_l for k in ["–±–∏—Ç–∫–æ–∏–Ω","bitcoin","btc","–∫—Ä–∏–ø—Ç","ethereum","eth","stablecoin"]): tadd("–∫—Ä–∏–ø—Ç–∞")
    if any(k in text_l for k in ["–¥–æ–ª–ª–∞—Ä","usd","–µ–≤—Ä–æ","eur","—Ä—É–±–ª","rub","—é–∞–Ω—å","cny","–∫—É—Ä—Å","—Ñ–æ—Ä–µ–∫—Å"]): tadd("–≤–∞–ª—é—Ç–∞")
    if any(k in text_l for k in ["–∞–∫—Ü–∏","—Ä—ã–Ω–æ–∫","–±–∏—Ä–∂","–∏–Ω–¥–µ–∫—Å","nasdaq","nyse","s&p","sp500","dow"]): tadd("—Ä—ã–Ω–∫–∏")
    if any(k in text_l for k in ["—Å—Ç–∞–≤–∫","—Ñ—Ä—Å","—Ü–±","–∏–Ω—Ñ–ª—è—Ü","cpi","ppi","qe","qt"]): tadd("—Å—Ç–∞–≤–∫–∏")
    if any(k in text_l for k in ["–Ω–µ—Ñ—Ç—å","–±—Ä–µ–Ω—Ç","wti","opec","–≥–∞–∑","—ç–Ω–µ—Ä–≥–∏","lng"]): tadd("—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
    if any(k in text_l for k in ["—Å–∞–Ω–∫—Ü","—ç–º–±–∞—Ä–≥–æ","–ø–æ—à–ª–∏–Ω","–≥–µ–æ–ø–æ–ª–∏—Ç","–ø–µ—Ä–µ–≥–æ–≤–æ—Ä","–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"]): tadd("–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞")
    nouns=extract_candidate_nouns(title+" "+body, entities, limit=12)
    result=[]
    def add(s):
        if s and s not in result: result.append(s)
    for t in thematic: add(t)
    for n in nouns: add(COUNTRY_PROPER.get(n.lower(), n))
    tags=[]
    for t in result:
        if re.fullmatch(r"[A-Z]{2,6}", t): tags.append("#"+t)
        else: tags.append("#"+(t if t in COUNTRY_PROPER.values() else t.lower()))
        if len(tags)>=max_tags: break
    if len(tags)<min_tags:
        for extra in ["#—Ä—ã–Ω–∫–∏","#–≤–∞–ª—é—Ç–∞","#–∫—Ä–∏–ø—Ç–∞","#—Å—Ç–∞–≤–∫–∏","#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞","#–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞"]:
            if extra not in tags: tags.append(extra)
            if len(tags)>=min_tags: break
    return "||"+" ".join(tags[:max_tags])+"||"

# ========= –ì—Ä–∞–¥–∏–µ–Ω—Ç (—è—Ä—á–µ ~30%) =========
PALETTES = [((32,44,80),(12,16,28)),((16,64,88),(8,20,36)),((82,30,64),(14,12,24)),
            ((20,88,72),(8,24,22)),((90,60,22),(20,16,12)),((44,22,90),(16,12,32)),((24,26,32),(12,14,18))]
def _boost(c, factor=1.3): return tuple(max(0, min(255, int(v*factor))) for v in c)
def random_gradient(w=1080, h=540):
    top,bottom = random.choice(PALETTES)
    top, bottom = _boost(top,1.3), _boost(bottom,1.3)
    angle = random.choice([0,15,30,45,60,75,90,120,135])
    img = Image.new("RGB",(w,h)); d=ImageDraw.Draw(img)
    steps = max(w,h)
    for i in range(steps):
        t = i/(steps-1)
        r = int(top[0]*(1-t) + bottom[0]*t)
        g = int(top[1]*(1-t) + bottom[1]*t)
        b = int(top[2]*(1-t) + bottom[2]*t)
        d.line([(i*w//steps,0),(i*w//steps,h)], fill=(r,g,b))
    if angle not in (90,270):
        img = img.rotate(angle, expand=False, resample=Image.BICUBIC)
    img = ImageEnhance.Contrast(img).enhance(1.15)
    img = ImageEnhance.Brightness(img).enhance(1.05)
    mask = Image.new("L",(w,h),0)
    md = ImageDraw.Draw(mask)
    md.ellipse([-w*0.2,-h*0.4,w*1.2,h*1.4], fill=210)
    mask = mask.filter(ImageFilter.GaussianBlur(radius=80))
    img = Image.composite(img, Image.new("RGB",(w,h),(0,0,0)), mask)
    return img

# ========= –†–µ—Ä–∞–π—Ç/–ø–∞—Ä—Å–∏–Ω–≥ =========
RU_TONE_REWRITE=[(r"\b—Å–∫–∞–∑–∞–ª(–∞|–∏)?\b","—Å–æ–æ–±—â–∏–ª\\1"),(r"\b–∑–∞—è–≤–∏–ª(–∞|–∏)?\b","–æ—Ç–º–µ—Ç–∏–ª\\1"),
                 (r"\b–ø–æ —Å–ª–æ–≤–∞–º\b","–ø–æ –¥–∞–Ω–Ω—ã–º"),(r"\b–ø–æ –º–Ω–µ–Ω–∏—é\b","—Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ü–µ–Ω–∫–∞–º"),
                 (r"\b–ø—Ä–∏–º–µ—Ä–Ω–æ\b","–ø–æ—Ä—è–¥–∫–∞"),(r"\b–æ—á–µ–Ω—å\b","—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ"),(r"\b—Å–∏–ª—å–Ω–æ\b","–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ")]
def ru_scientific_paraphrase(s):
    out=s
    for pat,repl in RU_TONE_REWRITE: out=re.sub(pat,repl,out,flags=re.IGNORECASE)
    out=re.sub(r"\s+%","%",out)
    return re.sub(r"\s+"," ",out).strip()
def split_sentences(text):
    text=re.sub(r"\s+"," ",text or "").strip()
    return re.split(r"(?<=[.!?])\s+", text) if text else []
def paraphrase_sentence_ru_or_en(s):
    if detect_lang(s)=="en":
        s=translate_hard_ru(s)
    return ru_scientific_paraphrase(s)

def one_context_emoji(context):
    t=(context or "").lower()
    if any(k in t for k in ["–±–∏—Ç–∫–æ–∏–Ω","crypto","btc","ethereum","–∫—Ä–∏–ø—Ç"]): return "ü™ô"
    if any(k in t for k in ["–∞–∫—Ü–∏","–∏–Ω–¥–µ–∫—Å","—Ä—ã–Ω–æ–∫","–±–∏—Ä–∂","nasdaq","nyse","s&p"]): return "üìà"
    if any(k in t for k in ["–¥–æ–ª–ª–∞—Ä","—Ä—É–±–ª","–≤–∞–ª—é—Ç","–∫—É—Ä—Å","–µ–≤—Ä–æ","—é–∞–Ω—å","usd","eur","cny"]): return "üíµ"
    if any(k in t for k in ["—Å—Ç–∞–≤–∫","—Ñ—Ä—Å","—Ü–±","—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫","–∏–Ω—Ñ–ª—è—Ü","cpi","ppi"]): return "üè¶"
    if any(k in t for k in ["–Ω–µ—Ñ—Ç—å","–±—Ä–µ–Ω—Ç","wti","opec","–≥–∞–∑","lng","—ç–Ω–µ—Ä–≥–∏"]): return "üõ¢Ô∏è"
    if any(k in t for k in ["–∑–æ–ª–æ—Ç–æ","xau","–º–µ—Ç–∞–ª–ª","—Å–µ—Ä–µ–±—Ä–æ"]): return "ü•á"
    if any(k in t for k in ["—Å–∞–Ω–∫—Ü","—ç–º–±–∞—Ä–≥–æ","–ø–æ—à–ª–∏–Ω","–≥–µ–æ–ø–æ–ª–∏—Ç","–ø–µ—Ä–µ–≥–æ–≤–æ—Ä","–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç"]): return "üèõÔ∏è"
    return "üì∞"

def fetch_article_text(url, max_chars=2600):
    try:
        r = requests.get(url, headers=UA, timeout=20)
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        ps = soup.find_all("p")
        chunks = []
        for p in ps:
            t = p.get_text(" ", strip=True)
            if not t or len(t) < 60:
                continue
            if any(x in t.lower() for x in ["javascript","cookie","–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å","—Ä–µ–∫–ª–∞–º–∞","cookies"]):
                continue
            chunks.append(t)
            if sum(len(c) for c in chunks) > max_chars:
                break
        return re.sub(r"\s+", " ", " ".join(chunks)).strip()
    except Exception:
        return ""

def build_three_paragraphs_scientific(title, article_text, feed_summary):
    base_raw = (article_text or "").strip() or (feed_summary or "").strip()
    base_ru = ensure_russian(base_raw)

    sents = [s for s in split_sentences(base_ru) if s]
    p1_src = sents[:2] or sents[:1]
    p2_src = sents[2:5] or sents[:1]
    p3_src = sents[5:8] or sents[1:3] or sents[:1]

    p1 = " ".join(paraphrase_sentence_ru_or_en(s) for s in p1_src)
    p2 = " ".join(paraphrase_sentence_ru_or_en(s) for s in p2_src)
    p3 = " ".join(paraphrase_sentence_ru_or_en(s) for s in p3_src)

    emoji = one_context_emoji(f"{title} {base_ru}")

    # –ê–∫–∫—É—Ä–∞—Ç–∏–º –∞–±–∑–∞—Ü—ã: –∑–∞–≥–ª–∞–≤–Ω–∞—è –±—É–∫–≤–∞, –±–∞–ª–∞–Ω—Å —Å–∫–æ–±–æ–∫/–∫–∞–≤—ã—á–µ–∫
    p1 = tidy_paragraph(f"{emoji} {p1}".strip())
    p2 = tidy_paragraph(p2.strip())
    p3 = tidy_paragraph(p3.strip())

    return p1, p2, p3

# ========= –†–µ–Ω–¥–µ—Ä –∫–∞—Ä—Ç–æ—á–∫–∏ =========
def wrap_text_by_width(draw, text, font, max_width, max_lines=5):
    words=(text or "").split(); lines=[]; cur=""
    for w in words:
        test=(cur+" "+w).strip()
        if draw.textlength(test,font=font)<=max_width: cur=test
        else:
            if cur:
                lines.append(cur)
                if len(lines)>=max_lines: return lines
            cur=w
    if cur and len(lines)<max_lines: lines.append(cur)
    return lines
def fit_title_in_box(draw, text, font_path, box_w, box_h, start=66, min_s=28, line_gap=8, max_lines=5):
    for size in range(start, min_s-1, -2):
        font=ImageFont.truetype(font_path,size)
        lines=wrap_text_by_width(draw,text,font,box_w,max_lines=max_lines)
        h=font.getbbox("Ag")[3]; total=len(lines)*h+(len(lines)-1)*line_gap
        if lines and total<=box_h: return font,lines
    font=ImageFont.truetype(font_path,min_s)
    lines=wrap_text_by_width(draw,text,font,box_w,max_lines=max_lines)
    return font,lines
def draw_title_card(title_text, src_domain, tzname, event_dt_utc, post_dt_utc):
    W,H=1080,540
    bg=random_gradient(W,H)
    overlay=Image.new("RGBA",(W,H),(0,0,0,0))
    ImageDraw.Draw(overlay).rounded_rectangle([40,110,W-40,H-90], radius=28, fill=(0,0,0,118))
    bg=Image.alpha_composite(bg.convert("RGBA"),overlay).convert("RGB")
    d=ImageDraw.Draw(bg)
    path_bold="/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"
    path_reg ="/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
    f_brand=ImageFont.truetype(path_bold,34)
    f_time =ImageFont.truetype(path_reg,22)
    f_small=ImageFont.truetype(path_reg,20)
    d.text((48,26),CHANNEL_NAME,fill=(255,255,255),font=f_brand)
    try: tz=ZoneInfo(tzname)
    except Exception: tz=ZoneInfo("UTC")
    ev=event_dt_utc.astimezone(tz).strftime("%d.%m %H:%M")
    po=post_dt_utc.astimezone(tz).strftime("%d.%m %H:%M")
    right=f"–ø–æ—Å—Ç: {po}"
    d.text((W-48-d.textlength(right,font=f_time),28),right,fill=(255,255,255),font=f_time)
    box_x,box_y=72,150
    box_w,box_h=W-2*box_x, H-box_y-120
    f_title,lines=fit_title_in_box(d,(title_text or "").strip(),path_bold,box_w,box_h, start=66,min_s=30,max_lines=5)
    y=box_y
    for ln in lines:
        d.text((box_x,y),ln,font=f_title,fill=(255,255,255))
        y+=f_title.getbbox("Ag")[3]+8
    d.text((72,H-64),f"source: {src_domain}  ‚Ä¢  —Å–æ–±—ã—Ç–∏–µ: {ev}",font=f_small,fill=(230,230,230))
    bio=io.BytesIO(); bg.save(bio,format="PNG",optimize=True); bio.seek(0); return bio

# ========= HTML caption: —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —É–º–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ =========
def html_escape(s: str) -> str:
    return (s or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def smart_join_and_trim(paragraphs, max_len=1024):
    raw = "\n\n".join([p for p in paragraphs if p])
    if len(raw) <= max_len:
        return raw
    cut = raw[:max_len]
    for sep in [". ", "! ", "? ", "‚Ä¶ ", ".\n", "!\n", "?\n", "‚Ä¶\n"]:
        pos = cut.rfind(sep)
        if pos != -1:
            return cut[:pos+1].rstrip()
    return cut[:-1].rstrip() + "‚Ä¶"

def build_full_caption(title, p1, p2, p3, link, hidden_tags):
    dom = root_domain(link) if link else "–∏—Å—Ç–æ—á–Ω–∏–∫"

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∂–∏—Ä–Ω—ã–º
    title_html = f"<b>{html_escape(title)}</b>"

    # –¢–µ–ª–æ ‚Äî –±–µ–∑ <br>, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫
    body_plain = smart_join_and_trim([p1, p2, p3], max_len=1024 - 220)
    body_html = html_escape(body_plain)  # –ø–µ—Ä–µ–Ω–æ—Å—ã —É–∂–µ \n\n

    footer = [
    f'–ò—Å—Ç–æ—á–Ω–∏–∫: <a href="{html_escape(link)}">{html_escape(dom)}</a>',
    f'ü™ô <a href="{html_escape(CHANNEL_LINK)}">{html_escape(CHANNEL_NAME)}</a>'
]


    caption_no_tags = f"{title_html}\n\n{body_html}\n\n" + "\n".join(footer)

    # —Å–∫—Ä—ã—Ç—ã–µ —Ç–µ–≥–∏ –∫–∞–∫ —Å–ø–æ–π–ª–µ—Ä
    if hidden_tags:
        inner = hidden_tags.strip("|")  # "||#a #b||" -> "#a #b"
        spoiler_html = f'\n\n<span class="tg-spoiler">{html_escape(inner)}</span>'
        if len(caption_no_tags + spoiler_html) <= 1024:
            return caption_no_tags + spoiler_html

    return caption_no_tags


def send_photo_with_caption(photo_bytes, caption):
    if not BOT_TOKEN:
        raise RuntimeError("–ù–µ—Ç BOT_TOKEN (–¥–æ–±–∞–≤—å —Å–µ–∫—Ä–µ—Ç—ã –≤ Settings ‚Üí Secrets ‚Üí Actions)")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto"
    files = {"photo": ("cover.png", photo_bytes, "image/png")}
    data = {"chat_id": CHANNEL_ID, "caption": caption, "parse_mode": "HTML"}
    r = requests.post(url, files=files, data=data, timeout=30)
    print("Telegram sendPhoto:", r.status_code, r.text[:200])
    r.raise_for_status()
    return r.json()

# ========= –°–±–æ—Ä —Ñ–∏–¥–æ–≤ =========
def collect_entries():
    items=[]
    for feed_url in RSS_FEEDS:
        try:
            fp=feedparser.parse(feed_url)
        except Exception:
            continue
        for e in fp.entries or []:
            link=getattr(e,"link","") or ""
            title=(getattr(e,"title","") or "").strip()
            summary=clean_html(getattr(e,"summary",getattr(e,"description","")))
            ts=getattr(e,"published",getattr(e,"updated","")) or ""
            try:
                dt=dtparse.parse(ts)
                if not dt.tzinfo: dt=dt.replace(tzinfo=timezone.utc)
                else: dt=dt.astimezone(timezone.utc)
            except Exception:
                dt=datetime(1970,1,1,tzinfo=timezone.utc)
            if dt.year<MIN_EVENT_YEAR: continue
            uid=hashlib.sha256((link+"|"+title+"|"+ts).encode("utf-8")).hexdigest()
            items.append({"feed":feed_url,"link":link,"title":title or "(no title)",
                          "summary":summary,"ts":ts,"dt":dt,"uid":uid})
    return items

# ========= –§–∏–ª—å—Ç—Ä "–ø—É—Å—Ç—ã—Ö" –Ω–æ–≤–æ—Å—Ç–µ–π =========
def is_low_quality(title_ru, p1, p2, p3, min_total=280):
    text = (p1 + " " + p2 + " " + p3).strip()
    if len(text) < min_total:
        return True
    core = re.sub(r"[¬´¬ª\"'‚Äù‚Äú]", "", title_ru).lower()
    dup_score = sum(1 for x in [p1,p2,p3] if core[:40] in x.lower())
    return dup_score >= 2

# ========= –ü—Ä–æ—Ü–µ—Å—Å –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ =========
def process_item(item, now_utc):
    link, title, feed_summary, event_dt = item["link"], item["title"], item["summary"], item["dt"]
    title_ru = ensure_russian(title)
    article_text = fetch_article_text(link, max_chars=2600)
    p1, p2, p3 = build_three_paragraphs_scientific(title_ru, article_text, ensure_russian(feed_summary))

    if is_low_quality(title_ru, p1, p2, p3):
        print("Skip low-quality item:", title_ru[:80])
        return None

    entities = extract_entities(title_ru, f"{p1} {p2} {p3}")
    hidden_tags = gen_hidden_tags(title_ru, f"{p1} {p2} {p3}", entities, min_tags=3, max_tags=5)

    card = draw_title_card(title_ru, domain(link or ""), TIMEZONE, event_dt, now_utc)
    caption = build_full_caption(title_ru, p1, p2, p3, link or "", hidden_tags)
    resp = send_photo_with_caption(card, caption)

    append_history({
        "uid": item["uid"], "title": title_ru, "link": link,
        "event_utc": event_dt.isoformat(), "posted_utc": now_utc.isoformat(),
        "tags": hidden_tags
    })
    print(f"Posted: {title_ru[:80]}")
    return resp

# ========= MAIN =========
def trim_posted(posted_set, keep_last=1500):
    if len(posted_set)<=keep_last: return posted_set
    return set(list(posted_set)[-keep_last:])

def main():
    state=load_state()
    posted=set(state.get("posted_uids", []))
    items=collect_entries()
    if not items:
        print("No entries."); return

    now_utc=datetime.now(timezone.utc)
    lookback_dt=now_utc - timedelta(minutes=LOOKBACK_MINUTES)
    fresh_cutoff=now_utc - timedelta(minutes=FRESH_WINDOW_MIN)
    fresh=[it for it in items if it["dt"]>=fresh_cutoff and it["dt"]>=lookback_dt and it["uid"] not in posted]
    fresh.sort(key=lambda x: x["dt"], reverse=True)

    to_post = fresh[:MAX_POSTS_PER_RUN]

    if not to_post and FALLBACK_ON_NO_FRESH:
        fallback_cutoff = now_utc - timedelta(minutes=FALLBACK_WINDOW_MIN)
        candidates = [it for it in items if it["uid"] not in posted and it["dt"] >= fallback_cutoff]
        candidates.sort(key=lambda x: x["dt"], reverse=True)
        to_post = candidates[:MAX_POSTS_PER_RUN]
        if to_post:
            print(f"Fallback used: took newest item(s) within {FALLBACK_WINDOW_MIN} min.")

    if not to_post and ALWAYS_POST:
        anyc = [it for it in items if it["uid"] not in posted]
        anyc.sort(key=lambda x: x["dt"], reverse=True)
        to_post = anyc[:MAX_POSTS_PER_RUN]
        if to_post:
            print("ALWAYS_POST used: took newest item.")

    if not to_post:
        print("Nothing to post."); return

    posted_any = False
    for it in to_post:
        try:
            resp = process_item(it, now_utc)
            if resp:
                posted.add(it["uid"])
                posted_any = True
            time.sleep(1.0)
        except Exception as e:
            print("Error sending:", e)

    if posted_any:
        state["posted_uids"]=list(trim_posted(posted))
        save_state(state)

if __name__=="__main__":
    main()
