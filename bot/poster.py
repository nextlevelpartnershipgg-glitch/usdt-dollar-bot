import os, io, json, time, textwrap, pathlib, hashlib, urllib.parse, sys, random, re
from datetime import datetime, timezone, timedelta
from dateutil import parser as dtparse
import feedparser, requests
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
from zoneinfo import ZoneInfo

# ============ –ù–ê–°–¢–†–û–ô–ö–ò ============
BOT_TOKEN  = os.environ.get("BOT_TOKEN")                   # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û: GitHub Secrets
CHANNEL_ID = os.environ.get("CHANNEL_ID", "@usdtdollarm")  # —Ç–≤–æ–π –∫–∞–Ω–∞–ª –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
TIMEZONE   = os.environ.get("TIMEZONE", "Europe/Zurich")   # —Ç–≤–æ–π —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å

# –°–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–∏—Ç—å –∑–∞ –∑–∞–ø—É—Å–∫ (–∞–Ω—Ç–∏-—Å–ø–∞–º)
MAX_POSTS_PER_RUN = int(os.environ.get("MAX_POSTS_PER_RUN", "5"))
# –° –∫–∞–∫–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ –±—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ (–º–∏–Ω—É—Ç)
LOOKBACK_MINUTES  = int(os.environ.get("LOOKBACK_MINUTES", "90"))

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏
RSS_FEEDS = [
    "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",  # –†–ë–ö
    "https://lenta.ru/rss/news",                          # Lenta.ru
    "https://www.gazeta.ru/export/rss/lenta.xml",         # –ì–∞–∑–µ—Ç–∞.ru
    "https://tass.ru/rss/v2.xml",                         # –¢–ê–°–°
    "https://www.kommersant.ru/RSS/news.xml",             # –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä
]

# –•—ç—à—Ç–µ–≥–∏
TAGS = "#–Ω–æ–≤–æ—Å—Ç–∏ #—Ä—ã–Ω–∫–∏ #—ç–∫–æ–Ω–æ–º–∏–∫–∞ #–∞–∫—Ü–∏–∏ #usdt #–¥–æ–ª–ª–∞—Ä"

# –§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤)
DATA_DIR = pathlib.Path("data"); DATA_DIR.mkdir(parents=True, exist_ok=True)
STATE_FILE = DATA_DIR / "state.json"

UA  = {"User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
UA_IMG = {"User-Agent":"Mozilla/5.0"}

# ===== –£–¢–ò–õ–ò–¢–´ =====
def load_state():
    if STATE_FILE.exists():
        return json.loads(STATE_FILE.read_text(encoding="utf-8"))
    return {}

def save_state(state):
    STATE_FILE.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def domain(url):
    return urllib.parse.urlparse(url).netloc.replace("www.", "") or "source"

def clean_html(html):
    if not html:
        return ""
    soup = BeautifulSoup(html, "html.parser")
    return " ".join(soup.get_text(separator=" ").split())

def clamp(s, n):
    s = (s or "").strip()
    return (s if len(s) <= n else s[:n-1] + "‚Ä¶")

def make_caption(title, long_text, link, ctx_lines=None):
    title = clamp(title, 200)
    summary = clamp(long_text, 850)  # —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–∞—è –≤—ã–∂–∏–º–∫–∞
    lines = [f"üíµ {title}", f"{summary}"]
    if ctx_lines:
        lines += ["", "üß≠ –ö–æ–Ω—Ç–µ–∫—Å—Ç:"] + ctx_lines
    lines += ["", f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫: {link}", TAGS]
    cap = "\n".join(lines)
    # –ª–∏–º–∏—Ç Telegram ~1024
    if len(cap) > 1024:
        over = len(cap) - 1024 + 3
        summary2 = clamp(summary[:-over] if over < len(summary) else summary, 820)
        lines[1] = summary2
        cap = "\n".join(lines)
    return cap

# ---------- –§–û–ù: Unsplash ‚Üí Picsum ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç ----------
def fetch_unsplash_image(query, w=1080, h=540, retries=3):
    for i in range(retries):
        try:
            seed = random.randint(0, 10_000_000)
            url = f"https://source.unsplash.com/{w}x{h}/?{urllib.parse.quote(query)}&sig={seed}"
            r = requests.get(url, headers=UA_IMG, timeout=25, allow_redirects=True)
            if r.status_code == 200:
                return Image.open(io.BytesIO(r.content)).convert("RGB")
            time.sleep(0.8 * (i+1))
        except Exception:
            time.sleep(0.8 * (i+1))
    return None

def fetch_picsum_image(w=1080, h=540):
    try:
        seed = random.randint(1, 10_000_000)
        url = f"https://picsum.photos/{w}/{h}?random={seed}"
        r = requests.get(url, headers=UA_IMG, timeout=20, allow_redirects=True)
        if r.status_code == 200:
            return Image.open(io.BytesIO(r.content)).convert("RGB")
    except Exception:
        pass
    return None

def gradient_fallback(w=1080, h=540):
    top = (24, 26, 28); bottom = (10, 12, 14)
    img = Image.new("RGB", (w, h))
    draw = ImageDraw.Draw(img)
    for y in range(h):
        a = y/(h-1)
        r = int(top[0]*(1-a) + bottom[0]*a)
        g = int(top[1]*(1-a) + bottom[1]*a)
        b = int(top[2]*(1-a) + bottom[2]*a)
        draw.line([(0,y),(w,y)], fill=(r,g,b))
    return img

# ---------- –í–´–î–ï–õ–ï–ù–ò–ï –ö–õ–Æ–ß–ï–í–û–ô –ü–ï–†–°–û–ù–´/–ü–†–ï–î–ú–ï–¢–ê –î–õ–Ø –§–û–ù–ê ----------
COMPANY_HINTS = [
    "Apple","Microsoft","Tesla","Meta","Google","Alphabet","Amazon","Nvidia","Samsung","Intel","Huawei",
    "–ì–∞–∑–ø—Ä–æ–º","–°–±–µ—Ä–±–∞–Ω–∫","–Ø–Ω–¥–µ–∫—Å","–†–æ—Å–Ω–µ—Ñ—Ç—å","–õ—É–∫–æ–π–ª","–ù–æ—Ä–Ω–∏–∫–µ–ª—å","–¢–∞—Ç–Ω–µ—Ñ—Ç—å","–ù–æ–≤–∞—Ç—ç–∫","–í–¢–ë"
]
TICKER_PAT = re.compile(r"\b[A-Z]{2,5}\b")  # USD, EUR, BTC, AAPL‚Ä¶

def extract_entities(title, summary):
    text = f"{title} {summary}".strip()
    # 1) –ü–∞—Ä—ã/—Ç—Ä–æ–π–∫–∏ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö —Å–ª–æ–≤ (–∏–º–µ–Ω–∞/—Ñ–∞–º–∏–ª–∏–∏/–±—Ä–µ–Ω–¥—ã)
    cap_names = re.findall(r"(?:[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+(?:\s+[A-Z–ê-–Ø–Å][a-z–∞-—è—ë]+){0,2})", text)
    # 2) –¢–∏–∫–µ—Ä—ã/–∫–æ–¥—ã –≤–∞–ª—é—Ç
    tickers = [m for m in TICKER_PAT.findall(text) if m not in ("NEWS","HTTP","HTTPS","HTML")]
    # 3) –Ø–≤–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
    companies = [c for c in COMPANY_HINTS if c.lower() in text.lower()]
    # –£–¥–∞–ª–∏–º —Å–æ–≤—Å–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
    stop = {"The","This","That","Economy","Market","Index","–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç","–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ","–†–æ—Å—Å–∏—è","–°–®–ê"}
    cap_names = [x for x in cap_names if x not in stop and len(x) > 2]
    # –°–æ–±–µ—Ä—ë–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∏–º–µ–Ω–∞ ‚Üí –∫–æ–º–ø–∞–Ω–∏–∏ ‚Üí —Ç–∏–∫–µ—Ä—ã
    out = []
    out += cap_names[:3]
    out += companies[:3]
    out += tickers[:3]
    # fallback
    if not out:
        out = ["finance", "market"]
    return out

def build_photo_query(entities):
    # –ï—Å–ª–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø–µ—Ä—Å–æ–Ω—É (–¥–≤–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏) ‚Äî –ø–æ—Ä—Ç—Ä–µ—Ç
    if entities:
        ent = entities[0]
        if len(ent.split()) >= 2 and all(w and w[0].isupper() for w in ent.split()):
            return f"portrait,{ent}"
    # –ò–Ω–∞—á–µ –ø—Ä–µ–¥–º–µ—Ç/–±—Ä–µ–Ω–¥
    return ",".join(entities[:3])

def get_background(title, summary, w=1080, h=540):
    entities = extract_entities(title, summary)
    query = build_photo_query(entities)
    img = fetch_unsplash_image(query, w, h) or fetch_picsum_image(w, h) or gradient_fallback(w, h)
    img = img.filter(ImageFilter.GaussianBlur(radius=0.5))
    img = ImageEnhance.Brightness(img).enhance(0.9)
    return img, query

# ---------- –ö–ê–†–¢–û–ß–ö–ê 1080x540 (–¢–û–õ–¨–ö–û –ó–ê–ì–û–õ–û–í–û–ö) ----------
def draw_card_title_only(title_text, src_domain, tzname, query_used):
    W, H = 1080, 540
    # –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –ø–æ–¥–ª–æ–∂–∏–º –ª—ë–≥–∫—É—é —Ç–µ–Ω—å-–ø–ª–∞—à–∫—É
    bg, _ = get_background(title_text, "", W, H)
    d = ImageDraw.Draw(bg)

    # –∑–∞—Ç–µ–º–Ω—è–µ–º —à–∏—Ä–æ–∫—É—é —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é –æ–±–ª–∞—Å—Ç—å
    overlay = Image.new("RGBA", (W, H), (0,0,0,0))
    od = ImageDraw.Draw(overlay)
    od.rounded_rectangle([40, 110, W-40, H-90], radius=28, fill=(0,0,0,110))
    bg = Image.alpha_composite(bg.convert("RGBA"), overlay).convert("RGB")
    d = ImageDraw.Draw(bg)

    # –®—Ä–∏—Ñ—Ç—ã
    font_brand  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 34)
    font_time   = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 26)
    font_title  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 58)
    font_small  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 22)

    # –í–µ—Ä—Ö: –±—Ä–µ–Ω–¥ + –≤—Ä–µ–º—è
    brand = "USDT=Dollar"
    d.text((48, 26), brand, fill=(255,255,255), font=font_brand)
    try:
        tz = ZoneInfo(tzname)
    except Exception:
        tz = ZoneInfo("UTC")
    now_str = datetime.now(tz).strftime("%d.%m %H:%M")
    d.text((W - 48 - d.textlength(now_str, font=font_time), 26), now_str, fill=(255,255,255), font=font_time)

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ (—Ç–æ–ª—å–∫–æ –æ–Ω)
    margin_x = 72
    y = 150
    for line in textwrap.wrap((title_text or "").strip(), width=28)[:4]:
        d.text((margin_x, y), line, font=font_title, fill=(255,255,255))
        y += 66

    # –ù–∏–∑: –∏—Å—Ç–æ—á–Ω–∏–∫ + –ø–æ–¥—Å–∫–∞–∑–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (—á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –∑–∞ —Ñ–æ–Ω –ø–æ–¥–æ–±—Ä–∞–ª—Å—è)
    src = f"source: {src_domain}"
    d.text((72, H - 58), src, font=font_small, fill=(225,225,225))
    # –º–æ–∂–Ω–æ —Ç–∏—Ö–æ –≤—ã–≤–µ—Å—Ç–∏ query_used (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ)
    # d.text((W - 72 - d.textlength(query_used, font=font_small), H - 58), query_used, font=font_small, fill=(200,200,200))

    bio = io.BytesIO()
    bg.save(bio, format="PNG", optimize=True)
    bio.seek(0)
    return bio

def send_photo(photo_bytes, caption):
    if not BOT_TOKEN:
        raise RuntimeError("–ù–µ—Ç BOT_TOKEN. –î–æ–±–∞–≤—å —Å–µ–∫—Ä–µ—Ç –≤ GitHub: Settings ‚Üí Secrets ‚Üí Actions ‚Üí BOT_TOKEN")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto"
    files = {"photo": ("cover.png", photo_bytes, "image/png")}
    data = {"chat_id": CHANNEL_ID, "caption": caption}
    r = requests.post(url, files=files, data=data, timeout=30)
    print("Telegram status:", r.status_code, r.text[:200])
    r.raise_for_status()
    return r.json()

# ---------- –¢–ï–ö–°–¢ –°–û –°–¢–†–ê–ù–ò–¶–´ ----------
def fetch_article_text(url, max_chars=2000):
    try:
        r = requests.get(url, headers=UA, timeout=20)
        if r.status_code != 200:
            return ""
        soup = BeautifulSoup(r.text, "html.parser")
        ps = soup.find_all("p")
        chunks = []
        for p in ps:
            t = p.get_text(" ", strip=True)
            if not t: continue
            if len(t) < 60: continue
            if any(x in t.lower() for x in ["javascript","cookie","–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å","—Ä–µ–∫–ª–∞–º–∞","cookies"]):
                continue
            chunks.append(t)
            if sum(len(c) for c in chunks) > max_chars:
                break
        text = " ".join(chunks)
        text = re.sub(r"\s+", " ", text).strip()
        return text
    except Exception:
        return ""

def expanded_summary(feed_summary, article_text, limit=900):
    base = (article_text or "").strip() or (feed_summary or "").strip()
    sents = re.split(r"(?<=[.!?])\s+", base)
    out = " ".join(sents[:5]).strip()
    return clamp(out, limit)

# ---------- –°–¢–†–ê–ù–ê + –ì–û–°.–õ–ò–¶–ê ----------
COUNTRIES = [
    ("–†–æ—Å—Å–∏—è", ["—Ä–æ—Å—Å–∏—è","—Ä—Ñ","–º–æ—Å–∫–≤–∞","—Ä—É–±–ª","–ø—É—Ç–∏","—Ä–æ—Å—Å–∏–π"], "Q159"),
    ("–°–®–ê", ["—Å—à–∞","—Å–æ–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —à—Ç","washington","–±–∞–π–¥–µ–Ω","–¥–æ–ª–ª–∞—Ä","—Ñ—Ä—Å","–±–µ–ª—ã–π –¥–æ–º"], "Q30"),
    ("–ö–∏—Ç–∞–π", ["–∫–∏—Ç–∞–π","–∫–Ω—Ä","–ø–µ–∫–∏–Ω","—Å–∏ —Ü–∑–∏–Ω—å–ø–∏–Ω","—à–∞–Ω—å—Ö–∞–π","yuan","cny"], "Q148"),
    ("–£–∫—Ä–∞–∏–Ω–∞", ["—É–∫—Ä–∞–∏–Ω","–∫–∏–µ–≤","kyiv","–∑–µ–ª–µ–Ω—Å–∫","–≥—Ä–∏–≤–Ω","uah"], "Q212"),
    ("–ì–µ—Ä–º–∞–Ω–∏—è", ["–≥–µ—Ä–º–∞–Ω","–±–µ—Ä–ª–∏–Ω","scholz","–µ–≤—Ä–æ","bundes"], "Q183"),
    ("–§—Ä–∞–Ω—Ü–∏—è", ["—Ñ—Ä–∞–Ω—Ü","–ø–∞—Ä–∏–∂","–º–∞–∫—Ä–æ–Ω","euro","elysee"], "Q142"),
    ("–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è", ["–±—Ä–∏—Ç–∞–Ω–∏","–±—Ä–∏—Ç–∞–Ω","–ª–æ–Ω–¥–æ–Ω","–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç","uk","king charles","–ø—Ä–µ–º—å–µ—Ä"], "Q145"),
    ("–ò—Ç–∞–ª–∏—è", ["–∏—Ç–∞–ª","—Ä–∏–º","meloni","euro","–∏—Ç–∞–ª—å—è–Ω"], "Q38"),
    ("–ò—Å–ø–∞–Ω–∏—è", ["–∏—Å–ø–∞–Ω","–º–∞–¥—Ä–∏–¥","sanchez","euro","–∏–±–µ—Ä–∏"], "Q29"),
    ("–Ø–ø–æ–Ω–∏—è", ["—è–ø–æ–Ω–∏","—Ç–æ–∫–∏–æ","yen","jpy","kishida"], "Q17"),
    ("–ò–Ω–¥–∏—è", ["–∏–Ω–¥–∏—è","–Ω—å—é-–¥–µ–ª–∏","rupee","modi","inr"], "Q668"),
    ("–¢—É—Ä—Ü–∏—è", ["—Ç—É—Ä—Ü–∏","–∞–Ω–∫–∞—Ä–∞","—ç—Ä–¥–æ–≥–∞–Ω","lira","try"], "Q43"),
    ("–ü–æ–ª—å—à–∞", ["–ø–æ–ª—å—à","–≤–∞—Ä—à–∞–≤–∞","zl","pln","tusk"], "Q36"),
    ("–ë–µ–ª–∞—Ä—É—Å—å", ["–±–µ–ª–∞—Ä—É—Å","–º–∏–Ω—Å–∫","–ª—É–∫–∞—à–µ–Ω–∫","byn","–±–µ–ª–æ—Ä—É—Å"], "Q184"),
    ("–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω", ["–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω","–∞—Å—Ç–∞–Ω","—Ç–µ–Ω–≥–µ","kzt","—Ç–æ–∫–∞–µ–≤"], "Q232"),
    ("–ò—Ä–∞–Ω", ["–∏—Ä–∞–Ω","—Ç–µ–≥–µ—Ä–∞–Ω","rial","irn","—Ä–∞–∏—Å–∏","—Ö–∞–º–µ–Ω–µ"], "Q794"),
    ("–ò–∑—Ä–∞–∏–ª—å", ["–∏–∑—Ä–∞–∏–ª","—Ç–µ–ª—å-–∞–≤–∏–≤","–Ω–µ—Ç–∞–Ω—å—è—Ö—É","—à–µ–∫–µ–ª","ils"], "Q801"),
    ("–û–ê–≠", ["–æ–∞—ç","—ç–º–∏—Ä–∞—Ç","–∞–±—É-–¥–∞–±–∏","–¥—É–±–∞–π","aed","dirham"], "Q878"),
    ("–°–∞—É–¥–æ–≤—Å–∫–∞—è –ê—Ä–∞–≤–∏—è", ["—Å–∞—É–¥","—Ä–∏–∞–¥","sar","saudi","–º–±—Å"], "Q851"),
    ("–ö–∞–Ω–∞–¥–∞", ["–∫–∞–Ω–∞–¥–∞","–æ—Ç—Ç–∞–≤–∞","cad","trudeau"], "Q16"),
    ("–ë—Ä–∞–∑–∏–ª–∏—è", ["–±—Ä–∞–∑–∏–ª","—Ä–∏–æ","—Ä–µ–∞–ª","lula","sao paulo"], "Q155"),
    ("–ú–µ–∫—Å–∏–∫–∞", ["–º–µ–∫—Å–∏–∫","–ø–µ—Å–æ","mxn","–æ–±—Ä–∞–¥–æ—Ä","lopez obrador"], "Q96"),
]

def detect_country(text):
    t = (text or "").lower()
    for name, keys, qid in COUNTRIES:
        if any(k in t for k in keys):
            return {"name": name, "qid": qid}
    return None

def wikidata_officials(qid):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (head_of_state, head_of_gov). –ë–µ–∑ –ø–∞–¥–µ–Ω–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö."""
    try:
        query = f"""
        SELECT ?hosLabel ?hogLabel WHERE {{
          OPTIONAL {{ wd:{qid} wdt:P35 ?hos. }}
          OPTIONAL {{ wd:{qid} wdt:P6  ?hog. }}
          SERVICE wikibase:label {{ bd:serviceParam wikibase:language "ru,en". }}
        }}
        """
        r = requests.get(
            "https://query.wikidata.org/sparql",
            params={"query": query, "format": "json"},
            headers={"Accept":"application/sparql-results+json","User-Agent":"usdtdollar-bot/1.0"},
            timeout=15
        )
        if r.status_code != 200:
            return (None, None)
        data = r.json()
        hos, hog = None, None
        for b in data.get("results", {}).get("bindings", []):
            if "hosLabel" in b and not hos:
                hos = b["hosLabel"]["value"]
            if "hogLabel" in b and not hog:
                hog = b["hogLabel"]["value"]
        return (hos, hog)
    except Exception:
        return (None, None)

# ---------- –°–ë–û–† –ò –ü–£–ë–õ–ò–ö–ê–¶–ò–Ø ----------
def collect_entries():
    items = []
    for feed_url in RSS_FEEDS:
        fp = feedparser.parse(feed_url)
        for e in fp.entries or []:
            link = getattr(e, "link", "") or ""
            title = (getattr(e, "title", "") or "").strip()
            summary = clean_html(getattr(e, "summary", getattr(e, "description", "")))
            ts = getattr(e, "published", getattr(e, "updated", "")) or ""
            try:
                dt = dtparse.parse(ts)
                if not dt.tzinfo:
                    dt = dt.replace(tzinfo=timezone.utc)
            except Exception:
                dt = datetime(1970,1,1, tzinfo=timezone.utc)
            uid = hashlib.sha256((link + "|" + title + "|" + ts).encode("utf-8")).hexdigest()
            items.append({
                "feed": feed_url,
                "link": link,
                "title": title or "(no title)",
                "summary": summary,
                "ts": ts,
                "dt": dt,
                "uid": uid,
            })
    return items

def process_item(link, title, feed_summary):
    # —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Å—Ç–∞—Ç—å–∏
    article_text = fetch_article_text(link, max_chars=2000)
    long_summary = expanded_summary(feed_summary, article_text, limit=900)

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç: —Å—Ç—Ä–∞–Ω–∞ –∏ –ª–∏–¥–µ—Ä—ã
    country_info = detect_country(f"{title} {feed_summary} {article_text}")
    ctx_lines = []
    if country_info:
        hos, hog = wikidata_officials(country_info["qid"])
        ctx_lines.append(f"üó∫Ô∏è –°—Ç—Ä–∞–Ω–∞: {country_info['name']}")
        if hos: ctx_lines.append(f"üë§ –ì–ª–∞–≤–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞: {hos}")
        if hog: ctx_lines.append(f"üë§ –ì–ª–∞–≤–∞ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞: {hog}")

    # –ü–æ–¥–ø–∏—Å—å (–ø–æ–¥—Ä–æ–±–Ω–æ), –∫–∞—Ä—Ç–∏–Ω–∫–∞ (—Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫)
    cap  = make_caption(title, long_summary, link or "", ctx_lines=ctx_lines)

    # –î–ª—è —Ñ–æ–Ω–∞ –ø–æ—Å—Ç–∞—Ä–∞–µ–º—Å—è –≤–∑—è—Ç—å –∫–ª—é—á–µ–≤—É—é –ø–µ—Ä—Å–æ–Ω—É/–ø—Ä–µ–¥–º–µ—Ç
    entities = extract_entities(title, long_summary)
    query = build_photo_query(entities)
    # –ø–µ—Ä–µ—Ä–∏—Å—É–µ–º —Ñ–æ–Ω –ø–æ query –∏ –Ω–∞–ª–æ–∂–∏–º –¢–û–õ–¨–ö–û –∑–∞–≥–æ–ª–æ–≤–æ–∫
    # (–∏—Å–ø–æ–ª—å–∑—É–µ–º draw_card_title_only, –∫–æ—Ç–æ—Ä—ã–π –≤–Ω—É—Ç—Ä–∏ —Ç–∞–∫–∂–µ –∑–∞—Ç–µ–º–Ω—è–µ—Ç)
    # –ü–æ–¥–º–µ–Ω–∏–º —Ñ–æ–Ω: —Å–æ–∑–¥–∞–¥–∏–º –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ —É—á—ë–ª—Å—è –Ω–æ–≤—ã–π query
    bg_img = fetch_unsplash_image(query, 1080, 540) or fetch_picsum_image(1080, 540) or gradient_fallback(1080, 540)
    bg_img = bg_img.filter(ImageFilter.GaussianBlur(radius=0.5))
    bg_img = ImageEnhance.Brightness(bg_img).enhance(0.9)

    # –ù–∞—Ä–∏—Å—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —ç—Ç–æ–º —Ñ–æ–Ω–µ
    def finalize_card(image):
        W, H = 1080, 540
        img = image.resize((W,H))
        overlay = Image.new("RGBA", (W, H), (0,0,0,0))
        od = ImageDraw.Draw(overlay)
        od.rounded_rectangle([40, 110, W-40, H-90], radius=28, fill=(0,0,0,110))
        img = Image.alpha_composite(img.convert("RGBA"), overlay).convert("RGB")
        d = ImageDraw.Draw(img)
        font_brand  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 34)
        font_time   = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 26)
        font_title  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 58)
        font_small  = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 22)
        brand = "USDT=Dollar"
        d.text((48, 26), brand, fill=(255,255,255), font=font_brand)
        try:
            tz = ZoneInfo(TIMEZONE)
        except Exception:
            tz = ZoneInfo("UTC")
        now_str = datetime.now(tz).strftime("%d.%m %H:%M")
        d.text((W - 48 - d.textlength(now_str, font=font_time), 26), now_str, fill=(255,255,255), font=font_time)
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        margin_x = 72
        y = 150
        for line in textwrap.wrap((title or "").strip(), width=28)[:4]:
            d.text((margin_x, y), line, font=font_title, fill=(255,255,255))
            y += 66
        # –ù–∏–∑: –¥–æ–º–µ–Ω
        src = f"source: {domain(link or '')}"
        d.text((72, H - 58), src, font=font_small, fill=(225,225,225))
        bio = io.BytesIO()
        img.save(bio, format="PNG", optimize=True)
        bio.seek(0)
        return bio

    card = finalize_card(bg_img)
    resp = send_photo(card, cap)
    print("Posted:", (title or "")[:80], "‚Üí", resp.get("ok", True), "| query:", query)

def trim_posted(posted_set, keep_last=600):
    if len(posted_set) <= keep_last:
        return posted_set
    return set(list(posted_set)[-keep_last:])

def main():
    state = load_state()
    posted = set(state.get("posted_uids", []))

    items = collect_entries()
    if not items:
        print("No entries found.")
        return

    now = datetime.now(timezone.utc)
    lookback_dt = now - timedelta(minutes=LOOKBACK_MINUTES)
    fresh = [it for it in items if it["dt"] >= lookback_dt and it["uid"] not in posted]

    fresh.sort(key=lambda x: x["dt"], reverse=True)
    to_post = fresh[:MAX_POSTS_PER_RUN]

    if not to_post:
        print("Nothing new to post within lookback window.")
        return

    for it in to_post:
        try:
            process_item(it["link"], it["title"], it["summary"])
            posted.add(it["uid"])
            time.sleep(1.0)
        except Exception as e:
            print("Error sending:", e)

    state["posted_uids"] = list(trim_posted(posted))
    save_state(state)

if __name__ == "__main__":
    main()
